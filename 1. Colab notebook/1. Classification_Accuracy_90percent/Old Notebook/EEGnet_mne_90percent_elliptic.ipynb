{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **E1-201 : Hardware Acceleration on Machine Learning**\n",
        "---\n",
        "## **Course Project :**\n",
        "### **Implementation of Brain Computer Interface(BCI) Decoder using Arduino Nano 33 BLE sense Lite**\n",
        "---\n",
        "**Authors :**\n",
        "1. Hitesh Pavan Oleti (SR.No.:19804, ESE, IISc)\n",
        "2. Anand Chauhan      (SR.No.:*****, ESE, IISc)\n",
        "\n",
        "**Submitted to :**\n",
        "\n",
        "Prof. Chetan Singh Thakur,\n",
        "\n",
        "NeuRonICS Lab, IISc.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ssIqE13ZYAI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow Version**"
      ],
      "metadata": {
        "id": "Bfuvw-mpaQK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WM6SA1JEJF7",
        "outputId": "72fa2185-c296-4a8a-8f2e-9f5cf0a02382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing MNE Package**\n",
        "\n",
        "(An open-source Python package for exploring, visualizing, and analyzing human neurophysiological data: MEG, EEG, sEEG, ECoG and NIRS)"
      ],
      "metadata": {
        "id": "nmtfYRFsaGvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xkm-h5X6zk2",
        "outputId": "c0877dca-1782-440b-855a-3098a5510cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.8/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.8/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyriemann in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyriemann) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyriemann) (1.7.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from pyriemann) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from pyriemann) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from pyriemann) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->pyriemann) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->pyriemann) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->pyriemann) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pyriemann) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "CJz4tJdhbB9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPsM6F5u6FAV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# mne imports\n",
        "import mne\n",
        "from mne import io\n",
        "from mne.datasets import sample\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Google Drive**"
      ],
      "metadata": {
        "id": "zwJzgpeebdag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "y59dZi9TEKtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EEGNet Model**"
      ],
      "metadata": {
        "id": "X6_0exRgbpo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EEGNet(nb_classes, Chans = 64, Samples = 128, \n",
        "             dropoutRate = 0.5, kernLength = 64, F1 = 8, \n",
        "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
        "    \"\"\" Keras Implementation of EEGNet\n",
        "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
        "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
        "    version (version v1 and v2 on arxiv). We strongly recommend using this\n",
        "    architecture as it performs much better and has nicer properties than\n",
        "    our earlier version. For example:\n",
        "        \n",
        "        1. Depthwise Convolutions to learn spatial filters within a \n",
        "        temporal convolution. The use of the depth_multiplier option maps \n",
        "        exactly to the number of spatial filters learned within a temporal\n",
        "        filter. This matches the setup of algorithms like FBCSP which learn \n",
        "        spatial filters within each filter in a filter-bank. This also limits \n",
        "        the number of free parameters to fit when compared to a fully-connected\n",
        "        convolution. \n",
        "        \n",
        "        2. Separable Convolutions to learn how to optimally combine spatial\n",
        "        filters across temporal bands. Separable Convolutions are Depthwise\n",
        "        Convolutions followed by (1x1) Pointwise Convolutions. \n",
        "        \n",
        "    \n",
        "    While the original paper used Dropout, we found that SpatialDropout2D \n",
        "    sometimes produced slightly better results for classification of ERP \n",
        "    signals. However, SpatialDropout2D significantly reduced performance \n",
        "    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n",
        "    the default Dropout in most cases.\n",
        "        \n",
        "    Assumes the input signal is sampled at 128Hz. If you want to use this model\n",
        "    for any other sampling rate you will need to modify the lengths of temporal\n",
        "    kernels and average pooling size in blocks 1 and 2 as needed (double the \n",
        "    kernel lengths for double the sampling rate, etc). Note that we haven't \n",
        "    tested the model performance with this rule so this may not work well. \n",
        "    \n",
        "    The model with default parameters gives the EEGNet-8,2 model as discussed\n",
        "    in the paper. This model should do pretty well in general, although it is\n",
        "\tadvised to do some model searching to get optimal performance on your\n",
        "\tparticular dataset.\n",
        "    We set F2 = F1 * D (number of input filters = number of output filters) for\n",
        "    the SeparableConv2D layer. We haven't extensively tested other values of this\n",
        "    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n",
        "    overcomplete). We believe the main parameters to focus on are F1 and D. \n",
        "    Inputs:\n",
        "        \n",
        "      nb_classes      : int, number of classes to classify\n",
        "      Chans, Samples  : number of channels and time points in the EEG data\n",
        "      dropoutRate     : dropout fraction\n",
        "      kernLength      : length of temporal convolution in first layer. We found\n",
        "                        that setting this to be half the sampling rate worked\n",
        "                        well in practice. For the SMR dataset in particular\n",
        "                        since the data was high-passed at 4Hz we used a kernel\n",
        "                        length of 32.     \n",
        "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
        "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
        "      D               : number of spatial filters to learn within each temporal\n",
        "                        convolution. Default: D = 2\n",
        "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
        "    \"\"\"\n",
        "    \n",
        "    if dropoutType == 'SpatialDropout2D':\n",
        "        dropoutType = SpatialDropout2D\n",
        "    elif dropoutType == 'Dropout':\n",
        "        dropoutType = Dropout\n",
        "    else:\n",
        "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
        "                         'or Dropout, passed as a string.')\n",
        "    \n",
        "    input1   = Input(shape = (Chans, Samples, 1))\n",
        "\n",
        "    ##################################################################\n",
        "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                                   input_shape = (Chans, Samples, 1),\n",
        "                                   use_bias = False)(input1)\n",
        "    block1       = BatchNormalization()(block1)\n",
        "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
        "                                   depth_multiplier = D,\n",
        "                                   depthwise_constraint = max_norm(1.))(block1)\n",
        "    block1       = BatchNormalization()(block1)\n",
        "    block1       = Activation('relu')(block1)\n",
        "    block1       = AveragePooling2D((1, 4))(block1)\n",
        "    block1       = dropoutType(dropoutRate)(block1)\n",
        "    \n",
        "    block2       = SeparableConv2D(F2, (1, 16),\n",
        "                                   use_bias = False, padding = 'same')(block1)\n",
        "    block2       = BatchNormalization()(block2)\n",
        "    block2       = Activation('relu')(block2)\n",
        "    block2       = AveragePooling2D((1, 8))(block2)\n",
        "    block2       = dropoutType(dropoutRate)(block2)\n",
        "        \n",
        "    flatten      = Flatten(name = 'flatten')(block2)\n",
        "    \n",
        "    dense        = Dense(nb_classes, name = 'dense', \n",
        "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
        "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
        "    \n",
        "    return Model(inputs=input1, outputs=softmax)"
      ],
      "metadata": {
        "id": "lrk2WhjVRx33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Input Dataset**"
      ],
      "metadata": {
        "id": "UTxx5AsQb1Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "EPOC_dataset = read_csv('/content/gdrive/MyDrive/HAOML project/mne_iir_elliptic_5thorder.csv')\n",
        "EPOC_dataset = EPOC_dataset.values\n",
        "display(pd.DataFrame(EPOC_dataset))\n",
        "chans = 60; samples = 151;\n",
        "row, col = EPOC_dataset.shape\n",
        "\n",
        "y =  EPOC_dataset[:,col-1]\n",
        "X = np.zeros([row,chans,samples])\n",
        "for i in range(row):\n",
        "  X[i,:,:] = np.reshape(EPOC_dataset[i,0:col-1],(chans,samples))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "-SjpIlivapW3",
        "outputId": "9853e85e-c8c2-436a-ad98-e98e90000ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6     \\\n",
              "0   -0.006216 -0.002473 -0.001460  0.000278  0.002166 -0.000691 -0.005169   \n",
              "1   -0.013908 -0.001546  0.003924  0.002744 -0.000030 -0.000442  0.001255   \n",
              "2   -0.009865 -0.008093 -0.000416  0.007732  0.006604  0.001789  0.004578   \n",
              "3   -0.021735 -0.004651  0.004099  0.005067  0.004111  0.004046  0.004371   \n",
              "4    0.004510  0.001443 -0.003337 -0.004331 -0.003892 -0.004020 -0.001465   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "282  0.005864 -0.004179 -0.001315  0.006821 -0.000653 -0.014626 -0.008492   \n",
              "283  0.004587  0.001885 -0.004905 -0.006425 -0.002929 -0.001749 -0.004911   \n",
              "284 -0.007533 -0.000786  0.005859  0.006119  0.001304 -0.002044 -0.004138   \n",
              "285 -0.011000 -0.002675  0.003632  0.006324  0.003787 -0.001937 -0.002754   \n",
              "286 -0.000197 -0.005962 -0.008499 -0.004855  0.003403  0.008957  0.005499   \n",
              "\n",
              "         7         8         9     ...      9051      9052      9053  \\\n",
              "0   -0.000554  0.011705  0.015283  ... -0.000588 -0.002020 -0.002192   \n",
              "1    0.003638  0.007079  0.010767  ... -0.001095  0.003595  0.006555   \n",
              "2    0.011065  0.009254 -0.000050  ... -0.009036 -0.011451 -0.012151   \n",
              "3    0.004783  0.004688  0.003471  ... -0.002036 -0.002812 -0.002126   \n",
              "4    0.003807  0.005183  0.001069  ... -0.009972 -0.004302  0.000416   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "282  0.012965  0.017966  0.000361  ... -0.002424 -0.002267 -0.004446   \n",
              "283 -0.003857  0.007078  0.017092  ... -0.001034 -0.002224 -0.002893   \n",
              "284 -0.007412 -0.006266  0.002945  ... -0.000488 -0.000299  0.001258   \n",
              "285  0.004395  0.010109  0.006383  ... -0.002402 -0.003627 -0.003759   \n",
              "286 -0.001612 -0.000809  0.007834  ...  0.001254 -0.000425 -0.001330   \n",
              "\n",
              "         9054      9055      9056      9057      9058      9059  9060  \n",
              "0   -0.002092 -0.001955 -0.000659  0.000963  0.000479 -0.001698   3.0  \n",
              "1    0.009162  0.009595  0.007354  0.005818  0.005819  0.003452   1.0  \n",
              "2   -0.009407 -0.005621 -0.003849 -0.002715  0.001215  0.007001   4.0  \n",
              "3   -0.000408  0.002377  0.004493  0.004046  0.002502  0.002879   2.0  \n",
              "4    0.004141  0.005437  0.004721  0.004818  0.006758  0.008330   3.0  \n",
              "..        ...       ...       ...       ...       ...       ...   ...  \n",
              "282 -0.005083 -0.003439 -0.002352 -0.002922 -0.003316 -0.002525   4.0  \n",
              "283 -0.000772  0.003160  0.005563  0.005493  0.004522  0.003104   2.0  \n",
              "284  0.003975  0.005977  0.004637 -0.000201 -0.004684 -0.004382   3.0  \n",
              "285 -0.001958  0.000874  0.003127  0.003313  0.001040 -0.001837   1.0  \n",
              "286  0.001251  0.004692  0.004329  0.000384 -0.003010 -0.003767   4.0  \n",
              "\n",
              "[287 rows x 9061 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43f2a944-9636-46d5-9349-25ec72dbd1e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9051</th>\n",
              "      <th>9052</th>\n",
              "      <th>9053</th>\n",
              "      <th>9054</th>\n",
              "      <th>9055</th>\n",
              "      <th>9056</th>\n",
              "      <th>9057</th>\n",
              "      <th>9058</th>\n",
              "      <th>9059</th>\n",
              "      <th>9060</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.006216</td>\n",
              "      <td>-0.002473</td>\n",
              "      <td>-0.001460</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>-0.000691</td>\n",
              "      <td>-0.005169</td>\n",
              "      <td>-0.000554</td>\n",
              "      <td>0.011705</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000588</td>\n",
              "      <td>-0.002020</td>\n",
              "      <td>-0.002192</td>\n",
              "      <td>-0.002092</td>\n",
              "      <td>-0.001955</td>\n",
              "      <td>-0.000659</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>-0.001698</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.013908</td>\n",
              "      <td>-0.001546</td>\n",
              "      <td>0.003924</td>\n",
              "      <td>0.002744</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>-0.000442</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.003638</td>\n",
              "      <td>0.007079</td>\n",
              "      <td>0.010767</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001095</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>0.006555</td>\n",
              "      <td>0.009162</td>\n",
              "      <td>0.009595</td>\n",
              "      <td>0.007354</td>\n",
              "      <td>0.005818</td>\n",
              "      <td>0.005819</td>\n",
              "      <td>0.003452</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009865</td>\n",
              "      <td>-0.008093</td>\n",
              "      <td>-0.000416</td>\n",
              "      <td>0.007732</td>\n",
              "      <td>0.006604</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.011065</td>\n",
              "      <td>0.009254</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009036</td>\n",
              "      <td>-0.011451</td>\n",
              "      <td>-0.012151</td>\n",
              "      <td>-0.009407</td>\n",
              "      <td>-0.005621</td>\n",
              "      <td>-0.003849</td>\n",
              "      <td>-0.002715</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.007001</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.021735</td>\n",
              "      <td>-0.004651</td>\n",
              "      <td>0.004099</td>\n",
              "      <td>0.005067</td>\n",
              "      <td>0.004111</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>0.004783</td>\n",
              "      <td>0.004688</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.002812</td>\n",
              "      <td>-0.002126</td>\n",
              "      <td>-0.000408</td>\n",
              "      <td>0.002377</td>\n",
              "      <td>0.004493</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.002879</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.004510</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>-0.003337</td>\n",
              "      <td>-0.004331</td>\n",
              "      <td>-0.003892</td>\n",
              "      <td>-0.004020</td>\n",
              "      <td>-0.001465</td>\n",
              "      <td>0.003807</td>\n",
              "      <td>0.005183</td>\n",
              "      <td>0.001069</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009972</td>\n",
              "      <td>-0.004302</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.004141</td>\n",
              "      <td>0.005437</td>\n",
              "      <td>0.004721</td>\n",
              "      <td>0.004818</td>\n",
              "      <td>0.006758</td>\n",
              "      <td>0.008330</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.005864</td>\n",
              "      <td>-0.004179</td>\n",
              "      <td>-0.001315</td>\n",
              "      <td>0.006821</td>\n",
              "      <td>-0.000653</td>\n",
              "      <td>-0.014626</td>\n",
              "      <td>-0.008492</td>\n",
              "      <td>0.012965</td>\n",
              "      <td>0.017966</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002424</td>\n",
              "      <td>-0.002267</td>\n",
              "      <td>-0.004446</td>\n",
              "      <td>-0.005083</td>\n",
              "      <td>-0.003439</td>\n",
              "      <td>-0.002352</td>\n",
              "      <td>-0.002922</td>\n",
              "      <td>-0.003316</td>\n",
              "      <td>-0.002525</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.001885</td>\n",
              "      <td>-0.004905</td>\n",
              "      <td>-0.006425</td>\n",
              "      <td>-0.002929</td>\n",
              "      <td>-0.001749</td>\n",
              "      <td>-0.004911</td>\n",
              "      <td>-0.003857</td>\n",
              "      <td>0.007078</td>\n",
              "      <td>0.017092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001034</td>\n",
              "      <td>-0.002224</td>\n",
              "      <td>-0.002893</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>0.005563</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>0.004522</td>\n",
              "      <td>0.003104</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>-0.007533</td>\n",
              "      <td>-0.000786</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.006119</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>-0.002044</td>\n",
              "      <td>-0.004138</td>\n",
              "      <td>-0.007412</td>\n",
              "      <td>-0.006266</td>\n",
              "      <td>0.002945</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000488</td>\n",
              "      <td>-0.000299</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.003975</td>\n",
              "      <td>0.005977</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>-0.004684</td>\n",
              "      <td>-0.004382</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>-0.011000</td>\n",
              "      <td>-0.002675</td>\n",
              "      <td>0.003632</td>\n",
              "      <td>0.006324</td>\n",
              "      <td>0.003787</td>\n",
              "      <td>-0.001937</td>\n",
              "      <td>-0.002754</td>\n",
              "      <td>0.004395</td>\n",
              "      <td>0.010109</td>\n",
              "      <td>0.006383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002402</td>\n",
              "      <td>-0.003627</td>\n",
              "      <td>-0.003759</td>\n",
              "      <td>-0.001958</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.003127</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>-0.001837</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>-0.000197</td>\n",
              "      <td>-0.005962</td>\n",
              "      <td>-0.008499</td>\n",
              "      <td>-0.004855</td>\n",
              "      <td>0.003403</td>\n",
              "      <td>0.008957</td>\n",
              "      <td>0.005499</td>\n",
              "      <td>-0.001612</td>\n",
              "      <td>-0.000809</td>\n",
              "      <td>0.007834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>-0.000425</td>\n",
              "      <td>-0.001330</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.004692</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>-0.003010</td>\n",
              "      <td>-0.003767</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 9061 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43f2a944-9636-46d5-9349-25ec72dbd1e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43f2a944-9636-46d5-9349-25ec72dbd1e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43f2a944-9636-46d5-9349-25ec72dbd1e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(287, 60, 151)\n",
            "(287,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train, Validation and Test split**"
      ],
      "metadata": {
        "id": "iMTJAlyxb8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernels, chans, samples = 1, 60, 151\n",
        "\n",
        "# take 50/25/25 percent of the data to train/validate/test\n",
        "X_train      = X[0:144,]\n",
        "Y_train      = y[0:144]\n",
        "X_validate   = X[144:216,]\n",
        "Y_validate   = y[144:216]\n",
        "X_test       = X[216:,]\n",
        "Y_test       = y[216:]\n"
      ],
      "metadata": {
        "id": "IgPRHt5NRIaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EEGNet model training**"
      ],
      "metadata": {
        "id": "DKGJ1LrqcG1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################# EEGNet portion ##################################\n",
        "\n",
        "# convert labels to one-hot encodings.\n",
        "Y_train      = np_utils.to_categorical(Y_train-1)\n",
        "Y_validate   = np_utils.to_categorical(Y_validate-1)\n",
        "Y_test       = np_utils.to_categorical(Y_test-1)\n",
        "print(Y_train.shape, 'train samples Y')\n",
        "print(Y_test.shape, 'test samples Y')\n",
        "\n",
        "\n",
        "# convert data to NHWC (trials, channels, samples, kernels) format. Data \n",
        "# contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
        "X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
        "X_validate   = X_validate.reshape(X_validate.shape[0], chans, samples, kernels)\n",
        "X_test       = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
        "   \n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "# model configurations may do better, but this is a good starting point)\n",
        "model = EEGNet(nb_classes = 4, Chans = chans, Samples = samples, \n",
        "               dropoutRate = 0.5, kernLength = 32, F1 = 8, D = 2, F2 = 16, \n",
        "               dropoutType = 'Dropout')\n",
        "\n",
        "# compile the model and set the optimizers\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# count number of parameters in the model\n",
        "numParams    = model.count_params()    \n",
        "\n",
        "# set a valid path for your system to record model checkpoints\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                               save_best_only=True)\n",
        "\n",
        "class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "################################################################################\n",
        "# fit the model. Due to very small sample sizes this can get\n",
        "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
        "# Riemannian geometry classification (below)\n",
        "################################################################################\n",
        "fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                        verbose = 2, validation_data=(X_validate, Y_validate),\n",
        "                        callbacks=[checkpointer], )#class_weight = class_weights)\n",
        "\n",
        "# load optimal weights\n",
        "model.load_weights('/tmp/checkpoint.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X81w8wy6RSmv",
        "outputId": "1dd3de8f-744e-4268-dc66-5858391b7cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144, 4) train samples Y\n",
            "(71, 4) test samples Y\n",
            "X_train shape: (144, 60, 151, 1)\n",
            "144 train samples\n",
            "71 test samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 1.38632, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 3s - loss: 1.4352 - accuracy: 0.1875 - val_loss: 1.3863 - val_accuracy: 0.2500 - 3s/epoch - 309ms/step\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 2: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3820 - accuracy: 0.2708 - val_loss: 1.3863 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 3: val_loss improved from 1.38632 to 1.38632, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.3874 - accuracy: 0.2847 - val_loss: 1.3863 - val_accuracy: 0.2639 - 1s/epoch - 157ms/step\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3794 - accuracy: 0.2917 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 5: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3701 - accuracy: 0.3056 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 154ms/step\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3777 - accuracy: 0.2778 - val_loss: 1.3863 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3763 - accuracy: 0.3125 - val_loss: 1.3863 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3778 - accuracy: 0.3125 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3706 - accuracy: 0.3542 - val_loss: 1.3864 - val_accuracy: 0.3056 - 1s/epoch - 152ms/step\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3671 - accuracy: 0.3125 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 11: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3605 - accuracy: 0.3125 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 154ms/step\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 12: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3734 - accuracy: 0.2778 - val_loss: 1.3864 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 13: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3672 - accuracy: 0.4028 - val_loss: 1.3865 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 14: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3576 - accuracy: 0.3472 - val_loss: 1.3865 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 15: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3482 - accuracy: 0.3542 - val_loss: 1.3865 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3590 - accuracy: 0.3958 - val_loss: 1.3866 - val_accuracy: 0.2639 - 1s/epoch - 154ms/step\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 17: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3679 - accuracy: 0.2708 - val_loss: 1.3865 - val_accuracy: 0.2222 - 1s/epoch - 153ms/step\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 18: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3531 - accuracy: 0.4097 - val_loss: 1.3865 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 19: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3268 - accuracy: 0.4583 - val_loss: 1.3867 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 20: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3336 - accuracy: 0.4722 - val_loss: 1.3869 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 21: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3357 - accuracy: 0.4097 - val_loss: 1.3868 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 22: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3313 - accuracy: 0.3750 - val_loss: 1.3869 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 23: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3102 - accuracy: 0.4306 - val_loss: 1.3867 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3219 - accuracy: 0.3958 - val_loss: 1.3868 - val_accuracy: 0.2361 - 1s/epoch - 154ms/step\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 25: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3008 - accuracy: 0.4514 - val_loss: 1.3867 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.3212 - accuracy: 0.4306 - val_loss: 1.3865 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 27: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.2908 - accuracy: 0.5625 - val_loss: 1.3866 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 28: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.2904 - accuracy: 0.5000 - val_loss: 1.3866 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.2935 - accuracy: 0.4375 - val_loss: 1.3866 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.38632\n",
            "9/9 - 1s - loss: 1.2662 - accuracy: 0.5139 - val_loss: 1.3863 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 31: val_loss improved from 1.38632 to 1.38603, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.2714 - accuracy: 0.4722 - val_loss: 1.3860 - val_accuracy: 0.2361 - 1s/epoch - 157ms/step\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 32: val_loss improved from 1.38603 to 1.38581, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.2691 - accuracy: 0.4722 - val_loss: 1.3858 - val_accuracy: 0.2361 - 1s/epoch - 158ms/step\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 33: val_loss improved from 1.38581 to 1.38576, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.2677 - accuracy: 0.4722 - val_loss: 1.3858 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 34: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.2473 - accuracy: 0.5069 - val_loss: 1.3859 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 35: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.2376 - accuracy: 0.4653 - val_loss: 1.3863 - val_accuracy: 0.2361 - 1s/epoch - 153ms/step\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 36: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.2445 - accuracy: 0.5000 - val_loss: 1.3874 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 37: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.1867 - accuracy: 0.5972 - val_loss: 1.3872 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 38: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.2112 - accuracy: 0.5069 - val_loss: 1.3868 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 39: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.2105 - accuracy: 0.5417 - val_loss: 1.3879 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 40: val_loss did not improve from 1.38576\n",
            "9/9 - 1s - loss: 1.1783 - accuracy: 0.5694 - val_loss: 1.3872 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 41: val_loss improved from 1.38576 to 1.38550, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.1761 - accuracy: 0.5486 - val_loss: 1.3855 - val_accuracy: 0.2361 - 1s/epoch - 155ms/step\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 42: val_loss improved from 1.38550 to 1.38537, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.1112 - accuracy: 0.6389 - val_loss: 1.3854 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 43: val_loss did not improve from 1.38537\n",
            "9/9 - 1s - loss: 1.1133 - accuracy: 0.6458 - val_loss: 1.3856 - val_accuracy: 0.2361 - 1s/epoch - 155ms/step\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 44: val_loss improved from 1.38537 to 1.38446, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.0966 - accuracy: 0.6597 - val_loss: 1.3845 - val_accuracy: 0.2361 - 1s/epoch - 155ms/step\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 45: val_loss did not improve from 1.38446\n",
            "9/9 - 1s - loss: 1.1078 - accuracy: 0.5972 - val_loss: 1.3856 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 46: val_loss improved from 1.38446 to 1.38172, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.0970 - accuracy: 0.6111 - val_loss: 1.3817 - val_accuracy: 0.2361 - 1s/epoch - 157ms/step\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 47: val_loss did not improve from 1.38172\n",
            "9/9 - 1s - loss: 1.0813 - accuracy: 0.6389 - val_loss: 1.3848 - val_accuracy: 0.2361 - 1s/epoch - 154ms/step\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 48: val_loss did not improve from 1.38172\n",
            "9/9 - 1s - loss: 0.9947 - accuracy: 0.7222 - val_loss: 1.3838 - val_accuracy: 0.2361 - 1s/epoch - 152ms/step\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 49: val_loss did not improve from 1.38172\n",
            "9/9 - 1s - loss: 1.0107 - accuracy: 0.7222 - val_loss: 1.3849 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 50: val_loss improved from 1.38172 to 1.38099, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 1.0146 - accuracy: 0.6389 - val_loss: 1.3810 - val_accuracy: 0.2361 - 1s/epoch - 158ms/step\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 51: val_loss did not improve from 1.38099\n",
            "9/9 - 1s - loss: 1.0211 - accuracy: 0.6458 - val_loss: 1.3832 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 52: val_loss improved from 1.38099 to 1.37986, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.9943 - accuracy: 0.6597 - val_loss: 1.3799 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 53: val_loss improved from 1.37986 to 1.37738, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.9814 - accuracy: 0.6458 - val_loss: 1.3774 - val_accuracy: 0.2500 - 1s/epoch - 155ms/step\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 54: val_loss did not improve from 1.37738\n",
            "9/9 - 1s - loss: 0.9047 - accuracy: 0.7569 - val_loss: 1.3816 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 55: val_loss improved from 1.37738 to 1.37590, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.9660 - accuracy: 0.6944 - val_loss: 1.3759 - val_accuracy: 0.2361 - 1s/epoch - 156ms/step\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 56: val_loss did not improve from 1.37590\n",
            "9/9 - 1s - loss: 0.9597 - accuracy: 0.6806 - val_loss: 1.3828 - val_accuracy: 0.2361 - 1s/epoch - 150ms/step\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 57: val_loss did not improve from 1.37590\n",
            "9/9 - 1s - loss: 0.9509 - accuracy: 0.6875 - val_loss: 1.3845 - val_accuracy: 0.2361 - 1s/epoch - 151ms/step\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 58: val_loss did not improve from 1.37590\n",
            "9/9 - 1s - loss: 0.9206 - accuracy: 0.7500 - val_loss: 1.3774 - val_accuracy: 0.2639 - 1s/epoch - 151ms/step\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 59: val_loss improved from 1.37590 to 1.37419, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.9025 - accuracy: 0.7014 - val_loss: 1.3742 - val_accuracy: 0.2639 - 1s/epoch - 155ms/step\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 60: val_loss improved from 1.37419 to 1.36933, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8771 - accuracy: 0.7708 - val_loss: 1.3693 - val_accuracy: 0.2500 - 1s/epoch - 157ms/step\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 61: val_loss did not improve from 1.36933\n",
            "9/9 - 1s - loss: 0.8830 - accuracy: 0.7292 - val_loss: 1.3697 - val_accuracy: 0.2639 - 1s/epoch - 151ms/step\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 62: val_loss improved from 1.36933 to 1.35673, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 2s - loss: 0.8573 - accuracy: 0.7639 - val_loss: 1.3567 - val_accuracy: 0.2500 - 2s/epoch - 231ms/step\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 63: val_loss improved from 1.35673 to 1.34972, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8394 - accuracy: 0.7847 - val_loss: 1.3497 - val_accuracy: 0.2639 - 1s/epoch - 158ms/step\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 64: val_loss did not improve from 1.34972\n",
            "9/9 - 1s - loss: 0.8255 - accuracy: 0.7639 - val_loss: 1.3512 - val_accuracy: 0.2500 - 1s/epoch - 151ms/step\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 65: val_loss improved from 1.34972 to 1.34233, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8048 - accuracy: 0.8056 - val_loss: 1.3423 - val_accuracy: 0.2778 - 1s/epoch - 155ms/step\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 66: val_loss improved from 1.34233 to 1.33882, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8024 - accuracy: 0.7500 - val_loss: 1.3388 - val_accuracy: 0.2639 - 1s/epoch - 157ms/step\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 67: val_loss improved from 1.33882 to 1.30001, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8020 - accuracy: 0.7569 - val_loss: 1.3000 - val_accuracy: 0.3889 - 1s/epoch - 158ms/step\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 68: val_loss did not improve from 1.30001\n",
            "9/9 - 1s - loss: 0.7810 - accuracy: 0.8333 - val_loss: 1.3041 - val_accuracy: 0.3472 - 1s/epoch - 154ms/step\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 69: val_loss improved from 1.30001 to 1.29257, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7942 - accuracy: 0.7778 - val_loss: 1.2926 - val_accuracy: 0.3750 - 1s/epoch - 159ms/step\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 70: val_loss improved from 1.29257 to 1.24698, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.8300 - accuracy: 0.7500 - val_loss: 1.2470 - val_accuracy: 0.4444 - 1s/epoch - 156ms/step\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 71: val_loss did not improve from 1.24698\n",
            "9/9 - 1s - loss: 0.7400 - accuracy: 0.8333 - val_loss: 1.2663 - val_accuracy: 0.4028 - 1s/epoch - 154ms/step\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 72: val_loss improved from 1.24698 to 1.24671, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7399 - accuracy: 0.8125 - val_loss: 1.2467 - val_accuracy: 0.4444 - 1s/epoch - 157ms/step\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 73: val_loss improved from 1.24671 to 1.24621, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7527 - accuracy: 0.8194 - val_loss: 1.2462 - val_accuracy: 0.4444 - 1s/epoch - 156ms/step\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 74: val_loss improved from 1.24621 to 1.20562, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7280 - accuracy: 0.8056 - val_loss: 1.2056 - val_accuracy: 0.5000 - 1s/epoch - 156ms/step\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 75: val_loss did not improve from 1.20562\n",
            "9/9 - 1s - loss: 0.7070 - accuracy: 0.8264 - val_loss: 1.2485 - val_accuracy: 0.4583 - 1s/epoch - 151ms/step\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 76: val_loss improved from 1.20562 to 1.18422, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7558 - accuracy: 0.8333 - val_loss: 1.1842 - val_accuracy: 0.5556 - 1s/epoch - 157ms/step\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 77: val_loss did not improve from 1.18422\n",
            "9/9 - 1s - loss: 0.6611 - accuracy: 0.8611 - val_loss: 1.1861 - val_accuracy: 0.5833 - 1s/epoch - 156ms/step\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 78: val_loss improved from 1.18422 to 1.17317, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.7185 - accuracy: 0.8194 - val_loss: 1.1732 - val_accuracy: 0.5278 - 1s/epoch - 157ms/step\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 79: val_loss improved from 1.17317 to 1.14008, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6443 - accuracy: 0.8681 - val_loss: 1.1401 - val_accuracy: 0.5972 - 1s/epoch - 155ms/step\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 80: val_loss improved from 1.14008 to 1.11534, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6940 - accuracy: 0.8472 - val_loss: 1.1153 - val_accuracy: 0.6389 - 1s/epoch - 157ms/step\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 81: val_loss improved from 1.11534 to 1.09843, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6463 - accuracy: 0.8958 - val_loss: 1.0984 - val_accuracy: 0.5833 - 1s/epoch - 156ms/step\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 82: val_loss did not improve from 1.09843\n",
            "9/9 - 1s - loss: 0.6646 - accuracy: 0.8472 - val_loss: 1.1018 - val_accuracy: 0.6250 - 1s/epoch - 152ms/step\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 83: val_loss did not improve from 1.09843\n",
            "9/9 - 1s - loss: 0.5953 - accuracy: 0.8819 - val_loss: 1.1212 - val_accuracy: 0.6111 - 1s/epoch - 151ms/step\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 84: val_loss improved from 1.09843 to 1.08883, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6732 - accuracy: 0.8403 - val_loss: 1.0888 - val_accuracy: 0.5972 - 1s/epoch - 157ms/step\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 85: val_loss improved from 1.08883 to 1.07833, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6337 - accuracy: 0.8750 - val_loss: 1.0783 - val_accuracy: 0.5694 - 1s/epoch - 164ms/step\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 86: val_loss improved from 1.07833 to 1.02205, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6936 - accuracy: 0.8264 - val_loss: 1.0221 - val_accuracy: 0.6806 - 1s/epoch - 155ms/step\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 87: val_loss improved from 1.02205 to 1.01316, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6383 - accuracy: 0.8542 - val_loss: 1.0132 - val_accuracy: 0.6111 - 1s/epoch - 156ms/step\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 88: val_loss improved from 1.01316 to 0.99860, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6308 - accuracy: 0.8542 - val_loss: 0.9986 - val_accuracy: 0.6944 - 1s/epoch - 155ms/step\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 89: val_loss improved from 0.99860 to 0.97505, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6057 - accuracy: 0.8611 - val_loss: 0.9750 - val_accuracy: 0.6667 - 1s/epoch - 155ms/step\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 90: val_loss improved from 0.97505 to 0.96984, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6266 - accuracy: 0.9167 - val_loss: 0.9698 - val_accuracy: 0.6806 - 1s/epoch - 157ms/step\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 91: val_loss improved from 0.96984 to 0.94969, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5649 - accuracy: 0.8819 - val_loss: 0.9497 - val_accuracy: 0.7500 - 1s/epoch - 157ms/step\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 92: val_loss did not improve from 0.94969\n",
            "9/9 - 1s - loss: 0.6242 - accuracy: 0.8681 - val_loss: 0.9730 - val_accuracy: 0.6806 - 1s/epoch - 155ms/step\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 93: val_loss improved from 0.94969 to 0.92626, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5888 - accuracy: 0.8750 - val_loss: 0.9263 - val_accuracy: 0.6667 - 1s/epoch - 157ms/step\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 94: val_loss did not improve from 0.92626\n",
            "9/9 - 1s - loss: 0.5677 - accuracy: 0.8750 - val_loss: 0.9423 - val_accuracy: 0.6806 - 1s/epoch - 153ms/step\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 95: val_loss did not improve from 0.92626\n",
            "9/9 - 1s - loss: 0.5977 - accuracy: 0.8889 - val_loss: 0.9296 - val_accuracy: 0.6944 - 1s/epoch - 151ms/step\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 96: val_loss improved from 0.92626 to 0.87786, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.6140 - accuracy: 0.8611 - val_loss: 0.8779 - val_accuracy: 0.7222 - 1s/epoch - 156ms/step\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 97: val_loss improved from 0.87786 to 0.87354, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5511 - accuracy: 0.8958 - val_loss: 0.8735 - val_accuracy: 0.7083 - 1s/epoch - 155ms/step\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 98: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5923 - accuracy: 0.9097 - val_loss: 0.9158 - val_accuracy: 0.6667 - 1s/epoch - 151ms/step\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 99: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5841 - accuracy: 0.8611 - val_loss: 0.9236 - val_accuracy: 0.6528 - 1s/epoch - 152ms/step\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 100: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5568 - accuracy: 0.8889 - val_loss: 0.8996 - val_accuracy: 0.6667 - 1s/epoch - 153ms/step\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 101: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.6165 - accuracy: 0.8611 - val_loss: 0.8989 - val_accuracy: 0.6667 - 1s/epoch - 151ms/step\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 102: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5900 - accuracy: 0.8681 - val_loss: 0.9152 - val_accuracy: 0.6528 - 1s/epoch - 152ms/step\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 103: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5214 - accuracy: 0.9097 - val_loss: 0.9238 - val_accuracy: 0.6944 - 1s/epoch - 151ms/step\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 104: val_loss did not improve from 0.87354\n",
            "9/9 - 1s - loss: 0.5311 - accuracy: 0.8819 - val_loss: 0.8756 - val_accuracy: 0.6806 - 1s/epoch - 150ms/step\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 105: val_loss improved from 0.87354 to 0.85514, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5188 - accuracy: 0.9375 - val_loss: 0.8551 - val_accuracy: 0.7222 - 1s/epoch - 156ms/step\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 106: val_loss improved from 0.85514 to 0.85338, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5413 - accuracy: 0.8611 - val_loss: 0.8534 - val_accuracy: 0.7639 - 1s/epoch - 156ms/step\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 107: val_loss improved from 0.85338 to 0.84720, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5324 - accuracy: 0.8958 - val_loss: 0.8472 - val_accuracy: 0.7639 - 1s/epoch - 157ms/step\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 108: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.4995 - accuracy: 0.9097 - val_loss: 0.8615 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 109: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.5709 - accuracy: 0.8472 - val_loss: 0.8509 - val_accuracy: 0.6944 - 1s/epoch - 150ms/step\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 110: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.5142 - accuracy: 0.9167 - val_loss: 0.8706 - val_accuracy: 0.6806 - 1s/epoch - 151ms/step\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 111: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.5755 - accuracy: 0.8611 - val_loss: 0.8526 - val_accuracy: 0.7222 - 1s/epoch - 152ms/step\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 112: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.5620 - accuracy: 0.8958 - val_loss: 0.9065 - val_accuracy: 0.6111 - 1s/epoch - 151ms/step\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 113: val_loss did not improve from 0.84720\n",
            "9/9 - 1s - loss: 0.5441 - accuracy: 0.8889 - val_loss: 0.8917 - val_accuracy: 0.6667 - 1s/epoch - 153ms/step\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 114: val_loss improved from 0.84720 to 0.83979, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.5250 - accuracy: 0.9444 - val_loss: 0.8398 - val_accuracy: 0.6806 - 1s/epoch - 161ms/step\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 115: val_loss did not improve from 0.83979\n",
            "9/9 - 1s - loss: 0.5244 - accuracy: 0.9097 - val_loss: 0.8498 - val_accuracy: 0.7083 - 1s/epoch - 155ms/step\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 116: val_loss improved from 0.83979 to 0.79660, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4736 - accuracy: 0.9444 - val_loss: 0.7966 - val_accuracy: 0.6944 - 1s/epoch - 156ms/step\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 117: val_loss improved from 0.79660 to 0.78345, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4843 - accuracy: 0.9028 - val_loss: 0.7835 - val_accuracy: 0.7083 - 1s/epoch - 156ms/step\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 118: val_loss did not improve from 0.78345\n",
            "9/9 - 1s - loss: 0.4869 - accuracy: 0.9375 - val_loss: 0.8094 - val_accuracy: 0.7083 - 1s/epoch - 153ms/step\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 119: val_loss did not improve from 0.78345\n",
            "9/9 - 1s - loss: 0.4672 - accuracy: 0.9306 - val_loss: 0.8221 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 120: val_loss did not improve from 0.78345\n",
            "9/9 - 1s - loss: 0.4730 - accuracy: 0.9097 - val_loss: 0.7999 - val_accuracy: 0.7917 - 1s/epoch - 151ms/step\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 121: val_loss did not improve from 0.78345\n",
            "9/9 - 1s - loss: 0.5017 - accuracy: 0.9236 - val_loss: 0.8455 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 122: val_loss did not improve from 0.78345\n",
            "9/9 - 1s - loss: 0.4755 - accuracy: 0.9236 - val_loss: 0.7970 - val_accuracy: 0.7083 - 1s/epoch - 152ms/step\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 123: val_loss improved from 0.78345 to 0.77734, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4455 - accuracy: 0.9444 - val_loss: 0.7773 - val_accuracy: 0.7778 - 1s/epoch - 157ms/step\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 124: val_loss improved from 0.77734 to 0.77525, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4905 - accuracy: 0.9028 - val_loss: 0.7753 - val_accuracy: 0.7778 - 1s/epoch - 156ms/step\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 125: val_loss improved from 0.77525 to 0.76334, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 2s - loss: 0.4565 - accuracy: 0.9167 - val_loss: 0.7633 - val_accuracy: 0.7639 - 2s/epoch - 174ms/step\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 126: val_loss did not improve from 0.76334\n",
            "9/9 - 3s - loss: 0.4518 - accuracy: 0.9097 - val_loss: 0.7784 - val_accuracy: 0.7778 - 3s/epoch - 281ms/step\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 127: val_loss did not improve from 0.76334\n",
            "9/9 - 2s - loss: 0.4876 - accuracy: 0.9028 - val_loss: 0.7951 - val_accuracy: 0.7500 - 2s/epoch - 275ms/step\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 128: val_loss did not improve from 0.76334\n",
            "9/9 - 2s - loss: 0.4690 - accuracy: 0.9097 - val_loss: 0.8002 - val_accuracy: 0.7778 - 2s/epoch - 274ms/step\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 129: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4412 - accuracy: 0.8958 - val_loss: 0.8066 - val_accuracy: 0.7778 - 1s/epoch - 165ms/step\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 130: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4813 - accuracy: 0.9167 - val_loss: 0.8437 - val_accuracy: 0.7222 - 1s/epoch - 152ms/step\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 131: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4642 - accuracy: 0.9236 - val_loss: 0.7986 - val_accuracy: 0.7500 - 1s/epoch - 151ms/step\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 132: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4548 - accuracy: 0.9236 - val_loss: 0.7644 - val_accuracy: 0.7639 - 1s/epoch - 151ms/step\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 133: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4579 - accuracy: 0.9236 - val_loss: 0.7780 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 134: val_loss did not improve from 0.76334\n",
            "9/9 - 1s - loss: 0.4162 - accuracy: 0.9306 - val_loss: 0.7749 - val_accuracy: 0.7778 - 1s/epoch - 152ms/step\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 135: val_loss improved from 0.76334 to 0.76256, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4448 - accuracy: 0.9306 - val_loss: 0.7626 - val_accuracy: 0.7361 - 1s/epoch - 158ms/step\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 136: val_loss did not improve from 0.76256\n",
            "9/9 - 1s - loss: 0.4395 - accuracy: 0.9097 - val_loss: 0.7962 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 137: val_loss did not improve from 0.76256\n",
            "9/9 - 1s - loss: 0.4185 - accuracy: 0.9375 - val_loss: 0.8333 - val_accuracy: 0.6944 - 1s/epoch - 151ms/step\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 138: val_loss improved from 0.76256 to 0.74880, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4538 - accuracy: 0.9028 - val_loss: 0.7488 - val_accuracy: 0.7917 - 1s/epoch - 157ms/step\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 139: val_loss improved from 0.74880 to 0.74027, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4309 - accuracy: 0.9375 - val_loss: 0.7403 - val_accuracy: 0.8056 - 1s/epoch - 156ms/step\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 140: val_loss improved from 0.74027 to 0.73327, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4418 - accuracy: 0.9236 - val_loss: 0.7333 - val_accuracy: 0.8056 - 1s/epoch - 157ms/step\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 141: val_loss improved from 0.73327 to 0.72434, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4147 - accuracy: 0.9236 - val_loss: 0.7243 - val_accuracy: 0.7639 - 1s/epoch - 157ms/step\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 142: val_loss did not improve from 0.72434\n",
            "9/9 - 1s - loss: 0.4644 - accuracy: 0.8889 - val_loss: 0.7694 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 143: val_loss did not improve from 0.72434\n",
            "9/9 - 1s - loss: 0.4256 - accuracy: 0.9375 - val_loss: 0.7449 - val_accuracy: 0.8194 - 1s/epoch - 155ms/step\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 144: val_loss did not improve from 0.72434\n",
            "9/9 - 1s - loss: 0.4218 - accuracy: 0.9444 - val_loss: 0.7643 - val_accuracy: 0.6944 - 1s/epoch - 153ms/step\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 145: val_loss did not improve from 0.72434\n",
            "9/9 - 1s - loss: 0.4518 - accuracy: 0.8958 - val_loss: 0.7744 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 146: val_loss did not improve from 0.72434\n",
            "9/9 - 1s - loss: 0.3822 - accuracy: 0.9306 - val_loss: 0.7744 - val_accuracy: 0.7361 - 1s/epoch - 153ms/step\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 147: val_loss improved from 0.72434 to 0.70455, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4223 - accuracy: 0.9097 - val_loss: 0.7046 - val_accuracy: 0.7917 - 1s/epoch - 158ms/step\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 148: val_loss improved from 0.70455 to 0.67854, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.4560 - accuracy: 0.9028 - val_loss: 0.6785 - val_accuracy: 0.7917 - 1s/epoch - 158ms/step\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 149: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3945 - accuracy: 0.9306 - val_loss: 0.7347 - val_accuracy: 0.7500 - 1s/epoch - 150ms/step\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 150: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.4173 - accuracy: 0.9097 - val_loss: 0.7317 - val_accuracy: 0.7778 - 1s/epoch - 153ms/step\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 151: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3844 - accuracy: 0.9514 - val_loss: 0.7260 - val_accuracy: 0.7778 - 1s/epoch - 153ms/step\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 152: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.4266 - accuracy: 0.9306 - val_loss: 0.7506 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 153: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.4316 - accuracy: 0.9167 - val_loss: 0.6821 - val_accuracy: 0.7639 - 1s/epoch - 153ms/step\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 154: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3684 - accuracy: 0.9167 - val_loss: 0.7096 - val_accuracy: 0.7778 - 1s/epoch - 152ms/step\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 155: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.4024 - accuracy: 0.9444 - val_loss: 0.7212 - val_accuracy: 0.7500 - 1s/epoch - 154ms/step\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 156: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3793 - accuracy: 0.9375 - val_loss: 0.7507 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 157: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3725 - accuracy: 0.9375 - val_loss: 0.7220 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 158: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3654 - accuracy: 0.9444 - val_loss: 0.7595 - val_accuracy: 0.7083 - 1s/epoch - 153ms/step\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 159: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3674 - accuracy: 0.9444 - val_loss: 0.7730 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 160: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3111 - accuracy: 0.9514 - val_loss: 0.8503 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 161: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3582 - accuracy: 0.9375 - val_loss: 0.7667 - val_accuracy: 0.7222 - 1s/epoch - 151ms/step\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 162: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3670 - accuracy: 0.9306 - val_loss: 0.7681 - val_accuracy: 0.7083 - 1s/epoch - 153ms/step\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 163: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3753 - accuracy: 0.9583 - val_loss: 0.6944 - val_accuracy: 0.8194 - 1s/epoch - 152ms/step\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 164: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3925 - accuracy: 0.9236 - val_loss: 0.7213 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 165: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3973 - accuracy: 0.9583 - val_loss: 0.7438 - val_accuracy: 0.8056 - 1s/epoch - 153ms/step\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 166: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3379 - accuracy: 0.9514 - val_loss: 0.7215 - val_accuracy: 0.7639 - 1s/epoch - 154ms/step\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 167: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3320 - accuracy: 0.9444 - val_loss: 0.7120 - val_accuracy: 0.7917 - 1s/epoch - 150ms/step\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 168: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3513 - accuracy: 0.9375 - val_loss: 0.7088 - val_accuracy: 0.7917 - 1s/epoch - 151ms/step\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 169: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3473 - accuracy: 0.9583 - val_loss: 0.6948 - val_accuracy: 0.7639 - 1s/epoch - 155ms/step\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 170: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3190 - accuracy: 0.9514 - val_loss: 0.6932 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 171: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3269 - accuracy: 0.9583 - val_loss: 0.7113 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 172: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3161 - accuracy: 0.9792 - val_loss: 0.6801 - val_accuracy: 0.8194 - 1s/epoch - 151ms/step\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 173: val_loss did not improve from 0.67854\n",
            "9/9 - 1s - loss: 0.3187 - accuracy: 0.9583 - val_loss: 0.6873 - val_accuracy: 0.8056 - 1s/epoch - 149ms/step\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 174: val_loss improved from 0.67854 to 0.65363, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.3545 - accuracy: 0.9167 - val_loss: 0.6536 - val_accuracy: 0.8194 - 1s/epoch - 160ms/step\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 175: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3576 - accuracy: 0.9375 - val_loss: 0.6547 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 176: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3316 - accuracy: 0.9583 - val_loss: 0.7224 - val_accuracy: 0.7917 - 1s/epoch - 154ms/step\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 177: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3385 - accuracy: 0.9375 - val_loss: 0.9630 - val_accuracy: 0.6250 - 1s/epoch - 152ms/step\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 178: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3523 - accuracy: 0.9306 - val_loss: 0.6856 - val_accuracy: 0.7778 - 1s/epoch - 152ms/step\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 179: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3118 - accuracy: 0.9514 - val_loss: 0.6753 - val_accuracy: 0.7917 - 1s/epoch - 153ms/step\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 180: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3302 - accuracy: 0.9583 - val_loss: 0.7344 - val_accuracy: 0.7361 - 1s/epoch - 153ms/step\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 181: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3558 - accuracy: 0.9306 - val_loss: 0.7219 - val_accuracy: 0.7639 - 1s/epoch - 153ms/step\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 182: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3231 - accuracy: 0.9583 - val_loss: 0.7688 - val_accuracy: 0.6944 - 1s/epoch - 157ms/step\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 183: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3481 - accuracy: 0.9306 - val_loss: 0.9268 - val_accuracy: 0.6389 - 1s/epoch - 152ms/step\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 184: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3931 - accuracy: 0.9028 - val_loss: 0.7126 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 185: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3432 - accuracy: 0.9514 - val_loss: 0.7007 - val_accuracy: 0.7639 - 1s/epoch - 154ms/step\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 186: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2915 - accuracy: 0.9583 - val_loss: 0.8681 - val_accuracy: 0.6806 - 1s/epoch - 153ms/step\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 187: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3392 - accuracy: 0.9236 - val_loss: 0.6578 - val_accuracy: 0.7778 - 1s/epoch - 152ms/step\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 188: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2612 - accuracy: 0.9792 - val_loss: 0.6970 - val_accuracy: 0.7361 - 1s/epoch - 153ms/step\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 189: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3451 - accuracy: 0.9444 - val_loss: 0.7821 - val_accuracy: 0.7222 - 1s/epoch - 157ms/step\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 190: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3132 - accuracy: 0.9583 - val_loss: 0.7079 - val_accuracy: 0.7917 - 1s/epoch - 154ms/step\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 191: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3062 - accuracy: 0.9583 - val_loss: 0.6822 - val_accuracy: 0.8056 - 1s/epoch - 155ms/step\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 192: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3346 - accuracy: 0.9514 - val_loss: 0.6945 - val_accuracy: 0.7639 - 1s/epoch - 154ms/step\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 193: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3160 - accuracy: 0.9583 - val_loss: 0.7493 - val_accuracy: 0.7639 - 1s/epoch - 153ms/step\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 194: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3013 - accuracy: 0.9722 - val_loss: 0.7130 - val_accuracy: 0.7917 - 1s/epoch - 153ms/step\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 195: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2807 - accuracy: 0.9583 - val_loss: 0.7465 - val_accuracy: 0.7361 - 1s/epoch - 154ms/step\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 196: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2788 - accuracy: 0.9861 - val_loss: 0.7227 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 197: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3386 - accuracy: 0.9583 - val_loss: 0.6924 - val_accuracy: 0.7639 - 1s/epoch - 155ms/step\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 198: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2570 - accuracy: 0.9722 - val_loss: 0.7707 - val_accuracy: 0.7639 - 1s/epoch - 155ms/step\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 199: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2989 - accuracy: 0.9792 - val_loss: 0.6750 - val_accuracy: 0.8333 - 1s/epoch - 156ms/step\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 200: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3331 - accuracy: 0.9444 - val_loss: 0.6740 - val_accuracy: 0.7917 - 1s/epoch - 152ms/step\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 201: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3026 - accuracy: 0.9514 - val_loss: 0.6821 - val_accuracy: 0.8056 - 1s/epoch - 155ms/step\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 202: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2735 - accuracy: 0.9583 - val_loss: 0.6908 - val_accuracy: 0.7639 - 1s/epoch - 154ms/step\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 203: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2686 - accuracy: 0.9722 - val_loss: 0.7501 - val_accuracy: 0.7778 - 1s/epoch - 153ms/step\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 204: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2881 - accuracy: 0.9375 - val_loss: 0.7296 - val_accuracy: 0.7639 - 1s/epoch - 151ms/step\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 205: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2594 - accuracy: 0.9514 - val_loss: 0.7154 - val_accuracy: 0.7639 - 1s/epoch - 155ms/step\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 206: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2767 - accuracy: 0.9653 - val_loss: 0.7275 - val_accuracy: 0.7917 - 1s/epoch - 151ms/step\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 207: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2891 - accuracy: 0.9583 - val_loss: 0.6879 - val_accuracy: 0.7639 - 1s/epoch - 151ms/step\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 208: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3009 - accuracy: 0.9653 - val_loss: 0.7891 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 209: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2741 - accuracy: 0.9722 - val_loss: 0.7529 - val_accuracy: 0.7083 - 1s/epoch - 152ms/step\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 210: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2648 - accuracy: 0.9722 - val_loss: 0.7232 - val_accuracy: 0.7778 - 1s/epoch - 150ms/step\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 211: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2605 - accuracy: 0.9931 - val_loss: 0.7554 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 212: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3199 - accuracy: 0.9028 - val_loss: 0.7762 - val_accuracy: 0.7361 - 1s/epoch - 154ms/step\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 213: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2489 - accuracy: 0.9931 - val_loss: 0.8029 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 214: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2731 - accuracy: 0.9722 - val_loss: 0.7387 - val_accuracy: 0.7500 - 1s/epoch - 154ms/step\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 215: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2898 - accuracy: 0.9653 - val_loss: 0.6893 - val_accuracy: 0.7917 - 1s/epoch - 151ms/step\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 216: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2181 - accuracy: 0.9861 - val_loss: 0.7519 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 217: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.3218 - accuracy: 0.9722 - val_loss: 0.7317 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 218: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2624 - accuracy: 0.9653 - val_loss: 0.6715 - val_accuracy: 0.7917 - 1s/epoch - 151ms/step\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 219: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2547 - accuracy: 0.9583 - val_loss: 0.6964 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 220: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2412 - accuracy: 0.9653 - val_loss: 0.7357 - val_accuracy: 0.7361 - 1s/epoch - 155ms/step\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 221: val_loss did not improve from 0.65363\n",
            "9/9 - 1s - loss: 0.2442 - accuracy: 0.9722 - val_loss: 0.7133 - val_accuracy: 0.7778 - 1s/epoch - 154ms/step\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 222: val_loss improved from 0.65363 to 0.64979, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.2637 - accuracy: 0.9792 - val_loss: 0.6498 - val_accuracy: 0.8056 - 1s/epoch - 160ms/step\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 223: val_loss improved from 0.64979 to 0.64277, saving model to /tmp/checkpoint.h5\n",
            "9/9 - 1s - loss: 0.2476 - accuracy: 0.9792 - val_loss: 0.6428 - val_accuracy: 0.8056 - 1s/epoch - 155ms/step\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 224: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2558 - accuracy: 0.9583 - val_loss: 0.6502 - val_accuracy: 0.8194 - 1s/epoch - 151ms/step\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 225: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.3066 - accuracy: 0.9236 - val_loss: 0.8324 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 226: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2983 - accuracy: 0.9375 - val_loss: 0.7861 - val_accuracy: 0.6806 - 1s/epoch - 151ms/step\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 227: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2726 - accuracy: 0.9444 - val_loss: 0.6969 - val_accuracy: 0.7917 - 1s/epoch - 150ms/step\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 228: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2880 - accuracy: 0.9583 - val_loss: 0.6600 - val_accuracy: 0.7917 - 1s/epoch - 154ms/step\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 229: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2505 - accuracy: 0.9583 - val_loss: 0.7896 - val_accuracy: 0.7222 - 1s/epoch - 151ms/step\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 230: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2562 - accuracy: 0.9653 - val_loss: 0.7471 - val_accuracy: 0.6944 - 1s/epoch - 154ms/step\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 231: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2110 - accuracy: 0.9931 - val_loss: 0.7086 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 232: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2698 - accuracy: 0.9653 - val_loss: 0.8008 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 233: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2869 - accuracy: 0.9444 - val_loss: 0.7988 - val_accuracy: 0.6944 - 1s/epoch - 150ms/step\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 234: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.3004 - accuracy: 0.9236 - val_loss: 0.8390 - val_accuracy: 0.6528 - 1s/epoch - 152ms/step\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 235: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2503 - accuracy: 0.9722 - val_loss: 0.7411 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 236: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2842 - accuracy: 0.9306 - val_loss: 0.6850 - val_accuracy: 0.7500 - 1s/epoch - 150ms/step\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 237: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2978 - accuracy: 0.9306 - val_loss: 0.7091 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 238: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.3132 - accuracy: 0.9306 - val_loss: 0.6901 - val_accuracy: 0.7500 - 1s/epoch - 151ms/step\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 239: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2672 - accuracy: 0.9653 - val_loss: 0.7334 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 240: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2597 - accuracy: 0.9514 - val_loss: 0.7720 - val_accuracy: 0.7361 - 1s/epoch - 153ms/step\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 241: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2370 - accuracy: 0.9792 - val_loss: 0.7551 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 242: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2585 - accuracy: 0.9583 - val_loss: 0.7591 - val_accuracy: 0.7361 - 1s/epoch - 153ms/step\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 243: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2413 - accuracy: 0.9653 - val_loss: 0.7440 - val_accuracy: 0.7639 - 1s/epoch - 157ms/step\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 244: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2165 - accuracy: 0.9722 - val_loss: 0.6832 - val_accuracy: 0.7917 - 1s/epoch - 152ms/step\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 245: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2241 - accuracy: 0.9792 - val_loss: 0.7189 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 246: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2491 - accuracy: 0.9583 - val_loss: 0.7581 - val_accuracy: 0.7500 - 1s/epoch - 153ms/step\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 247: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2792 - accuracy: 0.9444 - val_loss: 0.9795 - val_accuracy: 0.6111 - 1s/epoch - 153ms/step\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 248: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2707 - accuracy: 0.9583 - val_loss: 0.7721 - val_accuracy: 0.7083 - 1s/epoch - 154ms/step\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 249: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.3049 - accuracy: 0.9306 - val_loss: 0.7782 - val_accuracy: 0.7778 - 1s/epoch - 155ms/step\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 250: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2348 - accuracy: 0.9653 - val_loss: 0.8115 - val_accuracy: 0.7639 - 1s/epoch - 153ms/step\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 251: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2414 - accuracy: 0.9514 - val_loss: 0.7326 - val_accuracy: 0.7639 - 1s/epoch - 156ms/step\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 252: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1996 - accuracy: 0.9931 - val_loss: 0.7750 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 253: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1975 - accuracy: 0.9931 - val_loss: 0.7838 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 254: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2239 - accuracy: 0.9792 - val_loss: 0.7872 - val_accuracy: 0.7361 - 1s/epoch - 154ms/step\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 255: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2410 - accuracy: 0.9792 - val_loss: 0.7626 - val_accuracy: 0.7778 - 1s/epoch - 156ms/step\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 256: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2611 - accuracy: 0.9444 - val_loss: 0.7736 - val_accuracy: 0.7778 - 1s/epoch - 155ms/step\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 257: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2808 - accuracy: 0.9653 - val_loss: 0.7039 - val_accuracy: 0.7500 - 1s/epoch - 154ms/step\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 258: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2479 - accuracy: 0.9653 - val_loss: 0.7692 - val_accuracy: 0.7639 - 1s/epoch - 155ms/step\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 259: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2461 - accuracy: 0.9653 - val_loss: 0.8067 - val_accuracy: 0.6944 - 1s/epoch - 154ms/step\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 260: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2064 - accuracy: 0.9722 - val_loss: 0.7863 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 261: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2758 - accuracy: 0.9306 - val_loss: 0.7443 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 262: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2309 - accuracy: 0.9792 - val_loss: 0.7526 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 263: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1987 - accuracy: 0.9792 - val_loss: 0.7040 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 264: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1795 - accuracy: 0.9722 - val_loss: 0.7212 - val_accuracy: 0.7639 - 1s/epoch - 151ms/step\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 265: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1960 - accuracy: 0.9861 - val_loss: 0.7785 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 266: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2451 - accuracy: 0.9653 - val_loss: 0.6745 - val_accuracy: 0.8333 - 1s/epoch - 156ms/step\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 267: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2365 - accuracy: 0.9514 - val_loss: 0.6990 - val_accuracy: 0.8056 - 1s/epoch - 152ms/step\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 268: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2599 - accuracy: 0.9444 - val_loss: 0.7957 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 269: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1975 - accuracy: 0.9722 - val_loss: 0.7107 - val_accuracy: 0.8194 - 1s/epoch - 152ms/step\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 270: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2471 - accuracy: 0.9444 - val_loss: 0.7374 - val_accuracy: 0.7361 - 1s/epoch - 151ms/step\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 271: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2446 - accuracy: 0.9722 - val_loss: 0.8820 - val_accuracy: 0.7222 - 1s/epoch - 151ms/step\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 272: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2832 - accuracy: 0.9583 - val_loss: 0.6910 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 273: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2541 - accuracy: 0.9514 - val_loss: 0.8086 - val_accuracy: 0.6944 - 1s/epoch - 151ms/step\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 274: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2692 - accuracy: 0.9375 - val_loss: 0.8449 - val_accuracy: 0.6944 - 1s/epoch - 155ms/step\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 275: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2759 - accuracy: 0.9583 - val_loss: 0.8724 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 276: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2910 - accuracy: 0.9306 - val_loss: 0.9850 - val_accuracy: 0.6389 - 1s/epoch - 151ms/step\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 277: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2397 - accuracy: 0.9722 - val_loss: 0.7999 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 278: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2339 - accuracy: 0.9792 - val_loss: 0.7608 - val_accuracy: 0.7639 - 1s/epoch - 154ms/step\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 279: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2196 - accuracy: 0.9583 - val_loss: 0.8162 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 280: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2344 - accuracy: 0.9583 - val_loss: 0.8551 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 281: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2581 - accuracy: 0.9583 - val_loss: 0.8332 - val_accuracy: 0.7083 - 1s/epoch - 152ms/step\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 282: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1958 - accuracy: 0.9722 - val_loss: 0.7704 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 283: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1979 - accuracy: 0.9722 - val_loss: 0.7694 - val_accuracy: 0.7500 - 1s/epoch - 150ms/step\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 284: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2188 - accuracy: 0.9583 - val_loss: 0.7835 - val_accuracy: 0.7222 - 1s/epoch - 152ms/step\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 285: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2334 - accuracy: 0.9792 - val_loss: 0.7201 - val_accuracy: 0.7361 - 1s/epoch - 152ms/step\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 286: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1935 - accuracy: 0.9722 - val_loss: 0.7026 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 287: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1920 - accuracy: 0.9792 - val_loss: 0.7993 - val_accuracy: 0.7222 - 1s/epoch - 154ms/step\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 288: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1472 - accuracy: 0.9931 - val_loss: 0.8037 - val_accuracy: 0.7500 - 1s/epoch - 151ms/step\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 289: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2107 - accuracy: 0.9583 - val_loss: 0.7388 - val_accuracy: 0.7222 - 1s/epoch - 154ms/step\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 290: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1810 - accuracy: 0.9722 - val_loss: 0.7417 - val_accuracy: 0.7222 - 1s/epoch - 152ms/step\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 291: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1913 - accuracy: 0.9792 - val_loss: 0.7633 - val_accuracy: 0.6944 - 1s/epoch - 153ms/step\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 292: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2029 - accuracy: 0.9722 - val_loss: 0.7567 - val_accuracy: 0.7778 - 1s/epoch - 150ms/step\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 293: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1891 - accuracy: 0.9792 - val_loss: 0.8253 - val_accuracy: 0.7083 - 1s/epoch - 151ms/step\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 294: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1782 - accuracy: 0.9722 - val_loss: 0.7766 - val_accuracy: 0.7639 - 1s/epoch - 150ms/step\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 295: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2026 - accuracy: 0.9722 - val_loss: 0.7909 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 296: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.2032 - accuracy: 0.9653 - val_loss: 0.7144 - val_accuracy: 0.7639 - 1s/epoch - 152ms/step\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 297: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1886 - accuracy: 1.0000 - val_loss: 0.6698 - val_accuracy: 0.7778 - 1s/epoch - 154ms/step\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 298: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1685 - accuracy: 0.9722 - val_loss: 0.6888 - val_accuracy: 0.7778 - 1s/epoch - 151ms/step\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 299: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1674 - accuracy: 1.0000 - val_loss: 0.6868 - val_accuracy: 0.7500 - 1s/epoch - 152ms/step\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 300: val_loss did not improve from 0.64277\n",
            "9/9 - 1s - loss: 0.1747 - accuracy: 0.9722 - val_loss: 0.7223 - val_accuracy: 0.7222 - 1s/epoch - 153ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification Accuracy**"
      ],
      "metadata": {
        "id": "YSa8doAfcOPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs       = model.predict(X_test)\n",
        "preds       = probs.argmax(axis = -1)  \n",
        "acc         = np.mean(preds == Y_test.argmax(axis=-1))\n",
        "print(\"Classification accuracy: %f \" % (acc))"
      ],
      "metadata": {
        "id": "xLMJ-jrJRY6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a7037c-e8ff-438c-f0c4-064d71d6d184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 40ms/step\n",
            "Classification accuracy: 0.887324 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "nLQq4IkncjE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrices for both classifiers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#Generate the confusion matrix\n",
        "plt.figure(figsize = (7,7))\n",
        "cf_matrix = confusion_matrix(Y_test.argmax(axis = -1), preds)\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Audio_left', 'Audio_right', 'Visual_left', 'Visual_right'])\n",
        "ax.yaxis.set_ticklabels(['Audio_left', 'Audio_right', 'Visual_left', 'Visual_right'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bFv0QSXORaM4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "399c4d51-f305-4e5e-cfe1-4588a670fe34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification accuracy: 0.732394 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe236fb5220>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define paths to model files**"
      ],
      "metadata": {
        "id": "MvSQ3pMfcz2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "MODELS_DIR = 'models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n"
      ],
      "metadata": {
        "id": "-n0uylCtUp3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining model name\n",
        "MODEL_TF = MODELS_DIR + 'model'\n",
        "MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\n",
        "tflite_model_name = \"model_tflite\"\n",
        "MODEL_TFLITE = MODELS_DIR + \"model.tflite\"\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'"
      ],
      "metadata": {
        "id": "oNXAqZNBUqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the model**"
      ],
      "metadata": {
        "id": "AtiOcisWc89R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(MODEL_TF)"
      ],
      "metadata": {
        "id": "F42T7W6BUzNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8736a8d-d538-4b90-a8e0-2261fbde746d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing features for evaluating models**"
      ],
      "metadata": {
        "id": "dp97qXFKdDTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_test = tf.convert_to_tensor(X_train,dtype=tf.float32)\n",
        "print(x_test.shape)\n",
        "def representative_dataset():\n",
        "  for i in range(144):\n",
        "    yield([tf.reshape(x_test[i],(1,60,151,1))])"
      ],
      "metadata": {
        "id": "cFQde_DoVBUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07430ecd-bab1-4d42-d63e-e49bcb277b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144, 60, 151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert the model to TFLite without quantization**"
      ],
      "metadata": {
        "id": "C5Aq0-uudH23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TFLite without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
        "model_no_quant_tflite = converter.convert()\n",
        "open(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)"
      ],
      "metadata": {
        "id": "2OhD5jcxWtSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cef658a-2496-4563-83eb-9d040d6f7595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12216"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convert and save the model with quantization**"
      ],
      "metadata": {
        "id": "YNxsPKvKdidS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert and save the model with quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Enforce integer only quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  #TFLITE_BUILTINS\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "# Create and provide a representative dataset\n",
        "converter.representative_dataset = representative_dataset\n",
        "model_tflite = converter.convert()\n",
        "open(MODEL_TFLITE, \"wb\").write(model_tflite)"
      ],
      "metadata": {
        "id": "x6qZSSzHWxNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f3db41-2cf3-436a-bad7-741a518a0fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8256"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape)\n",
        "x_test1 = X_test.astype(np.float32)\n",
        "print(x_test1.shape)"
      ],
      "metadata": {
        "id": "mKJJejehrB1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76d13c0-8869-4e7f-b687-73fb834f8a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(71, 60, 151)\n",
            "(71, 60, 151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TFLite with no Quantization**"
      ],
      "metadata": {
        "id": "aM1f09A1duqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize and allocate memory\n",
        "no_q_interpreter = tf.lite.Interpreter(model_content=model_no_quant_tflite)\n",
        "no_q_interpreter.allocate_tensors()\n",
        "\n",
        "y_test_pred_no_quant_tflite = np.empty([71,4], dtype=np.float32)\n",
        "success = 0;res_pred = [];res_act = [];\n",
        "\n",
        "for i in range(len(x_test1)):\n",
        "  #Input Tensor\n",
        "  input_shape = no_q_interpreter.get_input_details()[0]['shape'];\n",
        "  inputtensor = np.array(x_test1[i].reshape(input_shape), dtype=np.float32)\n",
        "  #Invoking model interpreter\n",
        "  no_q_interpreter.set_tensor(no_q_interpreter.get_input_details()[0][\"index\"], inputtensor)\n",
        "  no_q_interpreter.invoke()\n",
        "  #Calculating the testing accuracy\n",
        "  y_test_pred_no_quant_tflite[i] = no_q_interpreter.get_tensor(no_q_interpreter.get_output_details()[0][\"index\"])[0]\n",
        "  maxElem = np.amax(y_test_pred_no_quant_tflite[i])\n",
        "  res1 = np.where(y_test_pred_no_quant_tflite[i] == maxElem)\n",
        "  maxElem = np.amax(Y_test[i])\n",
        "  res2 = np.where(Y_test[i] == maxElem)\n",
        "  res_pred.append(res1[0]+1);\n",
        "  res_act.append(res2[0]+1);\n",
        "  if(res1[0] == res2[0]):\n",
        "    success+=1;\n",
        "\n",
        "#Displaying the predicted and actual labels\n",
        "pred = pd.DataFrame(np.reshape(x_test1,(71,60*151)));\n",
        "pred = pred.assign(Pred_Class1 = y_test_pred_no_quant_tflite[:,0],\n",
        "                   Pred_Class2 = y_test_pred_no_quant_tflite[:,1],\n",
        "                   Pred_Class3 = y_test_pred_no_quant_tflite[:,2],\n",
        "                   Pred_Class4 = y_test_pred_no_quant_tflite[:,3],\n",
        "                   Predicted_Class = res_pred,\n",
        "                   Actual_class = res_act);\n",
        "\n",
        "display(pred);\n",
        "\n",
        "print(\"\\nClassification Accuracy for non_quantized model = \",(success/len(x_test1)));    "
      ],
      "metadata": {
        "id": "JEDv44J7rJQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b09c766-4400-4ac6-b084-fb5450872374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2168254  0.7115437  0.02593151 0.04569944] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.0205219  0.01910297 0.9312277  0.02914745] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.55689204 0.1264193  0.22625491 0.09043378] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.02872921 0.07132661 0.01541379 0.8845304 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.11256393 0.6604397  0.1181263  0.10887009] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.03176183 0.0148222  0.9175828  0.0358332 ] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.6249292  0.2439071  0.01647037 0.11469332] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.01467727 0.9362018  0.02989528 0.01922563] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.01136822 0.01362455 0.9592025  0.01580461] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.7705577  0.08027067 0.11808384 0.03108779] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.11262685 0.05524831 0.12429978 0.7078251 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.30020598 0.44984177 0.0363699  0.21358237] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.02840637 0.00691668 0.95509773 0.00957925] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.85187376 0.07649722 0.0547892  0.01683976] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.02068449 0.01806556 0.08361941 0.8776306 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.08249331 0.7939435  0.06887996 0.05468323] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.02875355 0.02455572 0.8799376  0.06675313] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.27145922 0.11768126 0.41057697 0.20028259] [2]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.00537898 0.0039431  0.00745488 0.983223  ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.13218385 0.2654464  0.01661053 0.5857593 ] [3]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.01781667 0.0180966  0.869023   0.09506364] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.6930475  0.09637159 0.11565132 0.09492954] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.01729031 0.02325052 0.04369068 0.9157685 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.02227362 0.7830987  0.09538437 0.09924328] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.09126276 0.11331005 0.70327866 0.09214856] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.00303563 0.00680469 0.00455004 0.9856097 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.01434301 0.9307212  0.03649403 0.01844181] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[9.5112890e-04 6.0068155e-03 9.8610038e-01 6.9417087e-03] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.7673457  0.19023423 0.01577888 0.02664127] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.03814263 0.05614507 0.09284458 0.8128677 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.5279316 0.2921088 0.0839174 0.0960422] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.00872684 0.02521282 0.9394489  0.02661145] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.74703485 0.10785247 0.05924375 0.08586903] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.10168802 0.19738461 0.18367182 0.51725554] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.00623937 0.96438384 0.01850973 0.01086699] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.04368815 0.02591334 0.9036231  0.02677547] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.9656231  0.01689394 0.01184566 0.00563734] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.02684206 0.09297219 0.02446882 0.8557169 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.12743482 0.67237586 0.09578976 0.10439952] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.04688846 0.01148166 0.88555944 0.05607042] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.30562815 0.43943024 0.07067183 0.18426979] [1]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.16991393 0.09653911 0.22531816 0.5082287 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.3024929  0.48200673 0.04781529 0.16768505] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.06447656 0.07180864 0.3403312  0.5233836 ] [3]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.97079605 0.0138697  0.00512045 0.01021377] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.1188326  0.09234431 0.09498838 0.6938347 ] [3]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.3383942  0.27445003 0.0901043  0.29705143] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.00426533 0.08704629 0.04634061 0.8623478 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.01478673 0.9472386  0.02067491 0.01729975] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.011075   0.00248691 0.9805397  0.0058984 ] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.8162162  0.0845193  0.04022354 0.0590409 ] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.00440275 0.0117225  0.01345955 0.97041523] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.24899334 0.48932448 0.15635741 0.10532475] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.08257379 0.03420938 0.8179005  0.06531635] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.89687353 0.01410899 0.01703488 0.07198257] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[8.5344701e-04 2.4840198e-03 8.9837739e-04 9.9576414e-01] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.29186973 0.5444556  0.09031722 0.07335742] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.11105541 0.05690378 0.757608   0.07443275] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.6954683  0.13163002 0.04048042 0.13242127] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.09474093 0.09527116 0.07881601 0.7311719 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.13926786 0.741327   0.04974124 0.06966385] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[2.7032243e-03 8.4870262e-04 9.9305528e-01 3.3928005e-03] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.61320245 0.3119672  0.01826499 0.05656536] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.01319067 0.06721593 0.02941874 0.8901746 ] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.68314815 0.2038792  0.05883691 0.05413574] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.7550138  0.13854438 0.06438198 0.04205987] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.0038386  0.00456351 0.00309425 0.98850363] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[0.21234816 0.42570442 0.12936914 0.23257823] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[0.01380293 0.03397306 0.9359294  0.01629461] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[0.7980786  0.06982564 0.10256147 0.02953433] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[0.00927825 0.0193368  0.01639723 0.95498776] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "Classification Accuracy = %f\n",
            " 0.8873239436619719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Details**"
      ],
      "metadata": {
        "id": "lgN9cQABem9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_q_interpreter.get_input_details()"
      ],
      "metadata": {
        "id": "ru_L40Dbr6Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f3a36b-16f8-4650-b680-a2369bab0f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'serving_default_input_1:0',\n",
              "  'index': 0,\n",
              "  'shape': array([  1,  60, 151,   1], dtype=int32),\n",
              "  'shape_signature': array([ -1,  60, 151,   1], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output details**"
      ],
      "metadata": {
        "id": "dSjepkrieqfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_q_interpreter.get_output_details()"
      ],
      "metadata": {
        "id": "caNmw2zgr6po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71de601-6546-4f58-bc8a-b366b8115e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'StatefulPartitionedCall:0',\n",
              "  'index': 20,\n",
              "  'shape': array([1, 4], dtype=int32),\n",
              "  'shape_signature': array([-1,  4], dtype=int32),\n",
              "  'dtype': numpy.float32,\n",
              "  'quantization': (0.0, 0),\n",
              "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "   'zero_points': array([], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TFLite with Quantization**"
      ],
      "metadata": {
        "id": "iWrkoSVGeuq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Allocating tensors**"
      ],
      "metadata": {
        "id": "jkEoREXQe413"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_interpreter = tf.lite.Interpreter(model_content=model_tflite)\n",
        "q_interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "1YX6hqWbr9BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model input details**"
      ],
      "metadata": {
        "id": "0RWa5qQUe-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#interpreter gives us information we'll need to prepare input and output data\n",
        "q_interpreter.get_input_details()"
      ],
      "metadata": {
        "id": "bxrG78NSr_iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ff28a8-1ae3-4e4b-9399-f1c9bd9333dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'serving_default_input_1:0',\n",
              "  'index': 0,\n",
              "  'shape': array([  1,  60, 151,   1], dtype=int32),\n",
              "  'shape_signature': array([ -1,  60, 151,   1], dtype=int32),\n",
              "  'dtype': numpy.int8,\n",
              "  'quantization': (0.0008163931197486818, -24),\n",
              "  'quantization_parameters': {'scales': array([0.00081639], dtype=float32),\n",
              "   'zero_points': array([-24], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model output details**"
      ],
      "metadata": {
        "id": "cFJX-_kvfDLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_interpreter.get_output_details()"
      ],
      "metadata": {
        "id": "iBNwEil-sEfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a1990e-09c2-428e-cbb2-15c37455f174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'StatefulPartitionedCall:0',\n",
              "  'index': 20,\n",
              "  'shape': array([1, 4], dtype=int32),\n",
              "  'shape_signature': array([-1,  4], dtype=int32),\n",
              "  'dtype': numpy.int8,\n",
              "  'quantization': (0.00390625, -128),\n",
              "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
              "   'zero_points': array([-128], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_details = q_interpreter.get_input_details()[0]\n",
        "output_details = q_interpreter.get_output_details()[0]\n",
        "\n",
        "input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "\n",
        "x_test_ = x_test1 / input_scale + input_zero_point\n",
        "display(pd.DataFrame(np.reshape(x_test1,(71,151*60))));\n",
        "display(pd.DataFrame(np.reshape(x_test_,(71,151*60))));\n",
        "\n",
        "print(x_test1[3]) \n",
        "print(x_test_[3])\n",
        "x_test_ = x_test_.astype(input_details[\"dtype\"])\n",
        "display(pd.DataFrame(np.reshape(x_test_,(71,151*60))));\n",
        "\n",
        "print(\"\\n\\n\\nQuantized output : \\n\")\n",
        "# Invoke the interpreter\n",
        "y_test_pred_tflite = np.empty([71,4], dtype=output_details[\"dtype\"])\n",
        "success = 0;\n",
        "res_pred = [];res_act=[];\n",
        "for i in range(len(x_test_)):\n",
        "    #Input tensors\n",
        "    input_shape= q_interpreter.get_input_details()[0]['shape'];\n",
        "    inputtensor = np.array(x_test_[i].reshape(input_shape), dtype=np.int8)\n",
        "    #invoke interpreter\n",
        "    q_interpreter.set_tensor(input_details[\"index\"], inputtensor)\n",
        "    q_interpreter.invoke()\n",
        "    #Calculate test accuracy\n",
        "    y_test_pred_tflite[i] = q_interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    maxElem = np.amax(y_test_pred_tflite[i])\n",
        "    res1 = np.where(y_test_pred_tflite[i] == maxElem)\n",
        "    maxElem = np.amax(Y_test[i])\n",
        "    res2 = np.where(Y_test[i] == maxElem)\n",
        "    res_pred.append(res1[0]+1);\n",
        "    res_act.append(res2[0]+1);\n",
        "    if(res1[0][0] == res2[0][0]):\n",
        "      success+=1;\n",
        "  \n",
        "# If required, dequantized the output layer (from integer to float)\n",
        "output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "y_test_pred_tflite = y_test_pred_tflite.astype(np.float32)\n",
        "y_test_pred_tflite1 = (y_test_pred_tflite - output_zero_point) * output_scale\n",
        "\n",
        "pred = pd.DataFrame(np.reshape(x_test_,(71,60*151)));\n",
        "pred = pred.assign(Pred_Class1_q = y_test_pred_tflite[:,0],\n",
        "                   Pred_Class2_q = y_test_pred_tflite[:,1],\n",
        "                   Pred_Class3_q = y_test_pred_tflite[:,2],\n",
        "                   Pred_Class4_q = y_test_pred_tflite[:,3],\n",
        "                   Pred_Class1 = y_test_pred_tflite1[:,0],\n",
        "                   Pred_Class2 = y_test_pred_tflite1[:,1],\n",
        "                   Pred_Class3 = y_test_pred_tflite1[:,2],\n",
        "                   Pred_Class4 = y_test_pred_tflite1[:,3],                   \n",
        "                   Predicted_Class = res_pred,\n",
        "                   Actual_class = res_act);\n",
        "display(pred);\n",
        "\n",
        "print(\"\\nClassification Accuracy for Quantized model = \",success/len(x_test_));"
      ],
      "metadata": {
        "id": "vlvkKBUcsFXz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8c0736c-e09c-4e69-a548-cf889eb94c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        0         1         2         3         4         5         6     \\\n",
              "0  -0.004194 -0.010093 -0.002690  0.006129  0.001920 -0.003861  0.003806   \n",
              "1  -0.009148  0.000074  0.005536  0.002669 -0.001757 -0.001108  0.003534   \n",
              "2   0.003523 -0.003579 -0.004180  0.003713  0.004538 -0.003673 -0.006542   \n",
              "3  -0.006072 -0.011219 -0.009342  0.001391  0.010780  0.011061  0.006249   \n",
              "4  -0.005842 -0.001565 -0.005338 -0.008810  0.000150  0.014113  0.016709   \n",
              "..       ...       ...       ...       ...       ...       ...       ...   \n",
              "66  0.005864 -0.004179 -0.001315  0.006821 -0.000653 -0.014626 -0.008492   \n",
              "67  0.004587  0.001885 -0.004905 -0.006425 -0.002929 -0.001749 -0.004911   \n",
              "68 -0.007533 -0.000786  0.005859  0.006119  0.001304 -0.002044 -0.004138   \n",
              "69 -0.011000 -0.002675  0.003632  0.006324  0.003787 -0.001937 -0.002754   \n",
              "70 -0.000197 -0.005962 -0.008499 -0.004855  0.003403  0.008957  0.005499   \n",
              "\n",
              "        7         8         9     ...      9050      9051      9052      9053  \\\n",
              "0   0.013079  0.007205 -0.004015  ...  0.003846  0.005853  0.008227  0.009976   \n",
              "1   0.008710  0.009906  0.001528  ... -0.000337  0.001757  0.002527 -0.000464   \n",
              "2  -0.001757 -0.001036 -0.005094  ... -0.006786 -0.003386 -0.003664 -0.003788   \n",
              "3   0.003308  0.003544  0.004578  ... -0.000598  0.001392  0.001735 -0.000227   \n",
              "4   0.008568  0.001217 -0.003970  ...  0.000908  0.003237  0.006262  0.006199   \n",
              "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
              "66  0.012965  0.017966  0.000361  ... -0.005147 -0.002424 -0.002267 -0.004446   \n",
              "67 -0.003857  0.007078  0.017092  ... -0.000593 -0.001034 -0.002224 -0.002893   \n",
              "68 -0.007412 -0.006266  0.002945  ...  0.000255 -0.000488 -0.000299  0.001258   \n",
              "69  0.004395  0.010109  0.006383  ... -0.001810 -0.002402 -0.003627 -0.003759   \n",
              "70 -0.001612 -0.000809  0.007834  ...  0.001321  0.001254 -0.000425 -0.001330   \n",
              "\n",
              "        9054      9055      9056      9057      9058      9059  \n",
              "0   0.010733  0.011035  0.011264  0.011276  0.010110  0.006639  \n",
              "1  -0.003520 -0.003809 -0.003141 -0.003361 -0.003479 -0.002894  \n",
              "2  -0.000702  0.001584  0.000052 -0.000914  0.002233  0.005140  \n",
              "3  -0.001615 -0.000902 -0.000208 -0.001314 -0.002220 -0.000696  \n",
              "4   0.004143  0.004929  0.008231  0.008506  0.004403  0.000744  \n",
              "..       ...       ...       ...       ...       ...       ...  \n",
              "66 -0.005083 -0.003439 -0.002352 -0.002922 -0.003316 -0.002525  \n",
              "67 -0.000772  0.003160  0.005563  0.005493  0.004522  0.003104  \n",
              "68  0.003975  0.005977  0.004637 -0.000201 -0.004684 -0.004382  \n",
              "69 -0.001958  0.000874  0.003127  0.003313  0.001040 -0.001837  \n",
              "70  0.001251  0.004692  0.004329  0.000384 -0.003010 -0.003767  \n",
              "\n",
              "[71 rows x 9060 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fb15c0e-228c-423f-89a4-cf0480e503dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9050</th>\n",
              "      <th>9051</th>\n",
              "      <th>9052</th>\n",
              "      <th>9053</th>\n",
              "      <th>9054</th>\n",
              "      <th>9055</th>\n",
              "      <th>9056</th>\n",
              "      <th>9057</th>\n",
              "      <th>9058</th>\n",
              "      <th>9059</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.004194</td>\n",
              "      <td>-0.010093</td>\n",
              "      <td>-0.002690</td>\n",
              "      <td>0.006129</td>\n",
              "      <td>0.001920</td>\n",
              "      <td>-0.003861</td>\n",
              "      <td>0.003806</td>\n",
              "      <td>0.013079</td>\n",
              "      <td>0.007205</td>\n",
              "      <td>-0.004015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.005853</td>\n",
              "      <td>0.008227</td>\n",
              "      <td>0.009976</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.011035</td>\n",
              "      <td>0.011264</td>\n",
              "      <td>0.011276</td>\n",
              "      <td>0.010110</td>\n",
              "      <td>0.006639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.009148</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.005536</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>-0.001757</td>\n",
              "      <td>-0.001108</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.008710</td>\n",
              "      <td>0.009906</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000337</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.002527</td>\n",
              "      <td>-0.000464</td>\n",
              "      <td>-0.003520</td>\n",
              "      <td>-0.003809</td>\n",
              "      <td>-0.003141</td>\n",
              "      <td>-0.003361</td>\n",
              "      <td>-0.003479</td>\n",
              "      <td>-0.002894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003523</td>\n",
              "      <td>-0.003579</td>\n",
              "      <td>-0.004180</td>\n",
              "      <td>0.003713</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>-0.003673</td>\n",
              "      <td>-0.006542</td>\n",
              "      <td>-0.001757</td>\n",
              "      <td>-0.001036</td>\n",
              "      <td>-0.005094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006786</td>\n",
              "      <td>-0.003386</td>\n",
              "      <td>-0.003664</td>\n",
              "      <td>-0.003788</td>\n",
              "      <td>-0.000702</td>\n",
              "      <td>0.001584</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>-0.000914</td>\n",
              "      <td>0.002233</td>\n",
              "      <td>0.005140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.006072</td>\n",
              "      <td>-0.011219</td>\n",
              "      <td>-0.009342</td>\n",
              "      <td>0.001391</td>\n",
              "      <td>0.010780</td>\n",
              "      <td>0.011061</td>\n",
              "      <td>0.006249</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000598</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>-0.001615</td>\n",
              "      <td>-0.000902</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>-0.001314</td>\n",
              "      <td>-0.002220</td>\n",
              "      <td>-0.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.005842</td>\n",
              "      <td>-0.001565</td>\n",
              "      <td>-0.005338</td>\n",
              "      <td>-0.008810</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.014113</td>\n",
              "      <td>0.016709</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>-0.003970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000908</td>\n",
              "      <td>0.003237</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.004143</td>\n",
              "      <td>0.004929</td>\n",
              "      <td>0.008231</td>\n",
              "      <td>0.008506</td>\n",
              "      <td>0.004403</td>\n",
              "      <td>0.000744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.005864</td>\n",
              "      <td>-0.004179</td>\n",
              "      <td>-0.001315</td>\n",
              "      <td>0.006821</td>\n",
              "      <td>-0.000653</td>\n",
              "      <td>-0.014626</td>\n",
              "      <td>-0.008492</td>\n",
              "      <td>0.012965</td>\n",
              "      <td>0.017966</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005147</td>\n",
              "      <td>-0.002424</td>\n",
              "      <td>-0.002267</td>\n",
              "      <td>-0.004446</td>\n",
              "      <td>-0.005083</td>\n",
              "      <td>-0.003439</td>\n",
              "      <td>-0.002352</td>\n",
              "      <td>-0.002922</td>\n",
              "      <td>-0.003316</td>\n",
              "      <td>-0.002525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.001885</td>\n",
              "      <td>-0.004905</td>\n",
              "      <td>-0.006425</td>\n",
              "      <td>-0.002929</td>\n",
              "      <td>-0.001749</td>\n",
              "      <td>-0.004911</td>\n",
              "      <td>-0.003857</td>\n",
              "      <td>0.007078</td>\n",
              "      <td>0.017092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000593</td>\n",
              "      <td>-0.001034</td>\n",
              "      <td>-0.002224</td>\n",
              "      <td>-0.002893</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>0.005563</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>0.004522</td>\n",
              "      <td>0.003104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-0.007533</td>\n",
              "      <td>-0.000786</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.006119</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>-0.002044</td>\n",
              "      <td>-0.004138</td>\n",
              "      <td>-0.007412</td>\n",
              "      <td>-0.006266</td>\n",
              "      <td>0.002945</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>-0.000488</td>\n",
              "      <td>-0.000299</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.003975</td>\n",
              "      <td>0.005977</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>-0.004684</td>\n",
              "      <td>-0.004382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-0.011000</td>\n",
              "      <td>-0.002675</td>\n",
              "      <td>0.003632</td>\n",
              "      <td>0.006324</td>\n",
              "      <td>0.003787</td>\n",
              "      <td>-0.001937</td>\n",
              "      <td>-0.002754</td>\n",
              "      <td>0.004395</td>\n",
              "      <td>0.010109</td>\n",
              "      <td>0.006383</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001810</td>\n",
              "      <td>-0.002402</td>\n",
              "      <td>-0.003627</td>\n",
              "      <td>-0.003759</td>\n",
              "      <td>-0.001958</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.003127</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>-0.001837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-0.000197</td>\n",
              "      <td>-0.005962</td>\n",
              "      <td>-0.008499</td>\n",
              "      <td>-0.004855</td>\n",
              "      <td>0.003403</td>\n",
              "      <td>0.008957</td>\n",
              "      <td>0.005499</td>\n",
              "      <td>-0.001612</td>\n",
              "      <td>-0.000809</td>\n",
              "      <td>0.007834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>-0.000425</td>\n",
              "      <td>-0.001330</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.004692</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>-0.003010</td>\n",
              "      <td>-0.003767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 9060 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fb15c0e-228c-423f-89a4-cf0480e503dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9fb15c0e-228c-423f-89a4-cf0480e503dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9fb15c0e-228c-423f-89a4-cf0480e503dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         0          1          2          3          4          5     \\\n",
              "0  -29.137039 -36.362480 -27.295340 -16.493134 -21.648361 -28.729836   \n",
              "1  -35.205406 -23.909374 -17.219456 -20.731228 -26.151718 -25.357119   \n",
              "2  -19.685162 -28.383699 -29.120316 -19.451626 -18.440819 -28.498863   \n",
              "3  -31.437506 -37.741982 -35.442417 -22.295681 -10.795369 -10.451432   \n",
              "4  -31.155977 -25.916651 -30.538074 -34.791191 -23.816345  -6.713240   \n",
              "..        ...        ...        ...        ...        ...        ...   \n",
              "66 -16.816681 -29.119293 -25.610407 -15.645281 -24.800394 -41.915642   \n",
              "67 -18.381786 -21.690687 -30.007990 -31.869856 -27.587837 -26.142235   \n",
              "68 -33.226624 -24.962269 -16.823147 -16.505192 -22.402119 -26.503523   \n",
              "69 -37.473419 -27.277040 -19.551641 -16.254246 -19.361628 -26.372822   \n",
              "70 -24.241844 -31.303322 -34.410553 -29.946802 -19.831509 -13.028464   \n",
              "\n",
              "         6          7          8          9     ...       9050       9051  \\\n",
              "0  -19.337490  -7.979755 -15.174804 -28.917528  ... -19.289581 -16.831141   \n",
              "1  -19.671129 -13.330683 -11.866022 -22.128363  ... -24.413240 -21.847712   \n",
              "2  -32.012852 -26.152599 -25.268524 -30.239056  ... -32.311855 -28.147625   \n",
              "3  -16.346180 -19.947824 -19.658644 -18.392403  ... -24.732182 -22.295086   \n",
              "4   -3.532871 -13.504597 -22.509642 -28.863323  ... -22.887993 -20.035509   \n",
              "..        ...        ...        ...        ...  ...        ...        ...   \n",
              "66 -34.401466  -8.118751  -1.992933 -23.557362  ... -30.304083 -26.969481   \n",
              "67 -30.015455 -28.724018 -15.329709  -3.063929  ... -24.726116 -25.266510   \n",
              "68 -29.068279 -33.079178 -31.674835 -20.392738  ... -23.688147 -24.597729   \n",
              "69 -27.372875 -18.616150 -11.617200 -16.181297  ... -26.216665 -26.941952   \n",
              "70 -17.264351 -25.974705 -24.990423 -14.404683  ... -22.381468 -22.464420   \n",
              "\n",
              "         9052       9053       9054       9055       9056       9057  \\\n",
              "0  -13.923024 -11.780873 -10.853089 -10.483060 -10.202947 -10.188504   \n",
              "1  -20.905064 -24.568256 -28.311382 -28.665178 -27.847158 -28.117153   \n",
              "2  -28.487989 -28.640150 -24.860331 -22.059273 -23.936913 -25.119083   \n",
              "3  -21.874628 -24.277740 -25.977898 -25.104706 -24.254194 -25.609329   \n",
              "4  -16.329355 -16.406935 -18.925335 -17.962475 -13.917876 -13.581429   \n",
              "..        ...        ...        ...        ...        ...        ...   \n",
              "66 -26.776814 -29.446301 -30.225807 -28.211956 -26.881302 -27.578844   \n",
              "67 -26.724190 -27.543428 -24.945576 -20.128887 -17.185829 -17.271118   \n",
              "68 -24.365816 -22.458652 -19.130550 -16.678251 -18.320456 -24.246315   \n",
              "69 -28.442326 -28.604067 -26.398375 -22.929688 -20.169323 -19.942419   \n",
              "70 -24.520542 -25.629250 -22.467657 -18.252396 -18.697529 -23.529217   \n",
              "\n",
              "         9058       9059  \n",
              "0  -11.616485 -15.867960  \n",
              "1  -28.261045 -27.545065  \n",
              "2  -21.264309 -17.704199  \n",
              "3  -26.719006 -24.852844  \n",
              "4  -18.606867 -23.088327  \n",
              "..        ...        ...  \n",
              "66 -28.061911 -27.092699  \n",
              "67 -18.461475 -20.198460  \n",
              "68 -29.737663 -29.366905  \n",
              "69 -22.726204 -26.250223  \n",
              "70 -27.686354 -28.613878  \n",
              "\n",
              "[71 rows x 9060 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72dc2bd7-6f97-49a1-bdb5-e816991c092d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9050</th>\n",
              "      <th>9051</th>\n",
              "      <th>9052</th>\n",
              "      <th>9053</th>\n",
              "      <th>9054</th>\n",
              "      <th>9055</th>\n",
              "      <th>9056</th>\n",
              "      <th>9057</th>\n",
              "      <th>9058</th>\n",
              "      <th>9059</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-29.137039</td>\n",
              "      <td>-36.362480</td>\n",
              "      <td>-27.295340</td>\n",
              "      <td>-16.493134</td>\n",
              "      <td>-21.648361</td>\n",
              "      <td>-28.729836</td>\n",
              "      <td>-19.337490</td>\n",
              "      <td>-7.979755</td>\n",
              "      <td>-15.174804</td>\n",
              "      <td>-28.917528</td>\n",
              "      <td>...</td>\n",
              "      <td>-19.289581</td>\n",
              "      <td>-16.831141</td>\n",
              "      <td>-13.923024</td>\n",
              "      <td>-11.780873</td>\n",
              "      <td>-10.853089</td>\n",
              "      <td>-10.483060</td>\n",
              "      <td>-10.202947</td>\n",
              "      <td>-10.188504</td>\n",
              "      <td>-11.616485</td>\n",
              "      <td>-15.867960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-35.205406</td>\n",
              "      <td>-23.909374</td>\n",
              "      <td>-17.219456</td>\n",
              "      <td>-20.731228</td>\n",
              "      <td>-26.151718</td>\n",
              "      <td>-25.357119</td>\n",
              "      <td>-19.671129</td>\n",
              "      <td>-13.330683</td>\n",
              "      <td>-11.866022</td>\n",
              "      <td>-22.128363</td>\n",
              "      <td>...</td>\n",
              "      <td>-24.413240</td>\n",
              "      <td>-21.847712</td>\n",
              "      <td>-20.905064</td>\n",
              "      <td>-24.568256</td>\n",
              "      <td>-28.311382</td>\n",
              "      <td>-28.665178</td>\n",
              "      <td>-27.847158</td>\n",
              "      <td>-28.117153</td>\n",
              "      <td>-28.261045</td>\n",
              "      <td>-27.545065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-19.685162</td>\n",
              "      <td>-28.383699</td>\n",
              "      <td>-29.120316</td>\n",
              "      <td>-19.451626</td>\n",
              "      <td>-18.440819</td>\n",
              "      <td>-28.498863</td>\n",
              "      <td>-32.012852</td>\n",
              "      <td>-26.152599</td>\n",
              "      <td>-25.268524</td>\n",
              "      <td>-30.239056</td>\n",
              "      <td>...</td>\n",
              "      <td>-32.311855</td>\n",
              "      <td>-28.147625</td>\n",
              "      <td>-28.487989</td>\n",
              "      <td>-28.640150</td>\n",
              "      <td>-24.860331</td>\n",
              "      <td>-22.059273</td>\n",
              "      <td>-23.936913</td>\n",
              "      <td>-25.119083</td>\n",
              "      <td>-21.264309</td>\n",
              "      <td>-17.704199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-31.437506</td>\n",
              "      <td>-37.741982</td>\n",
              "      <td>-35.442417</td>\n",
              "      <td>-22.295681</td>\n",
              "      <td>-10.795369</td>\n",
              "      <td>-10.451432</td>\n",
              "      <td>-16.346180</td>\n",
              "      <td>-19.947824</td>\n",
              "      <td>-19.658644</td>\n",
              "      <td>-18.392403</td>\n",
              "      <td>...</td>\n",
              "      <td>-24.732182</td>\n",
              "      <td>-22.295086</td>\n",
              "      <td>-21.874628</td>\n",
              "      <td>-24.277740</td>\n",
              "      <td>-25.977898</td>\n",
              "      <td>-25.104706</td>\n",
              "      <td>-24.254194</td>\n",
              "      <td>-25.609329</td>\n",
              "      <td>-26.719006</td>\n",
              "      <td>-24.852844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-31.155977</td>\n",
              "      <td>-25.916651</td>\n",
              "      <td>-30.538074</td>\n",
              "      <td>-34.791191</td>\n",
              "      <td>-23.816345</td>\n",
              "      <td>-6.713240</td>\n",
              "      <td>-3.532871</td>\n",
              "      <td>-13.504597</td>\n",
              "      <td>-22.509642</td>\n",
              "      <td>-28.863323</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.887993</td>\n",
              "      <td>-20.035509</td>\n",
              "      <td>-16.329355</td>\n",
              "      <td>-16.406935</td>\n",
              "      <td>-18.925335</td>\n",
              "      <td>-17.962475</td>\n",
              "      <td>-13.917876</td>\n",
              "      <td>-13.581429</td>\n",
              "      <td>-18.606867</td>\n",
              "      <td>-23.088327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-16.816681</td>\n",
              "      <td>-29.119293</td>\n",
              "      <td>-25.610407</td>\n",
              "      <td>-15.645281</td>\n",
              "      <td>-24.800394</td>\n",
              "      <td>-41.915642</td>\n",
              "      <td>-34.401466</td>\n",
              "      <td>-8.118751</td>\n",
              "      <td>-1.992933</td>\n",
              "      <td>-23.557362</td>\n",
              "      <td>...</td>\n",
              "      <td>-30.304083</td>\n",
              "      <td>-26.969481</td>\n",
              "      <td>-26.776814</td>\n",
              "      <td>-29.446301</td>\n",
              "      <td>-30.225807</td>\n",
              "      <td>-28.211956</td>\n",
              "      <td>-26.881302</td>\n",
              "      <td>-27.578844</td>\n",
              "      <td>-28.061911</td>\n",
              "      <td>-27.092699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-18.381786</td>\n",
              "      <td>-21.690687</td>\n",
              "      <td>-30.007990</td>\n",
              "      <td>-31.869856</td>\n",
              "      <td>-27.587837</td>\n",
              "      <td>-26.142235</td>\n",
              "      <td>-30.015455</td>\n",
              "      <td>-28.724018</td>\n",
              "      <td>-15.329709</td>\n",
              "      <td>-3.063929</td>\n",
              "      <td>...</td>\n",
              "      <td>-24.726116</td>\n",
              "      <td>-25.266510</td>\n",
              "      <td>-26.724190</td>\n",
              "      <td>-27.543428</td>\n",
              "      <td>-24.945576</td>\n",
              "      <td>-20.128887</td>\n",
              "      <td>-17.185829</td>\n",
              "      <td>-17.271118</td>\n",
              "      <td>-18.461475</td>\n",
              "      <td>-20.198460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-33.226624</td>\n",
              "      <td>-24.962269</td>\n",
              "      <td>-16.823147</td>\n",
              "      <td>-16.505192</td>\n",
              "      <td>-22.402119</td>\n",
              "      <td>-26.503523</td>\n",
              "      <td>-29.068279</td>\n",
              "      <td>-33.079178</td>\n",
              "      <td>-31.674835</td>\n",
              "      <td>-20.392738</td>\n",
              "      <td>...</td>\n",
              "      <td>-23.688147</td>\n",
              "      <td>-24.597729</td>\n",
              "      <td>-24.365816</td>\n",
              "      <td>-22.458652</td>\n",
              "      <td>-19.130550</td>\n",
              "      <td>-16.678251</td>\n",
              "      <td>-18.320456</td>\n",
              "      <td>-24.246315</td>\n",
              "      <td>-29.737663</td>\n",
              "      <td>-29.366905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-37.473419</td>\n",
              "      <td>-27.277040</td>\n",
              "      <td>-19.551641</td>\n",
              "      <td>-16.254246</td>\n",
              "      <td>-19.361628</td>\n",
              "      <td>-26.372822</td>\n",
              "      <td>-27.372875</td>\n",
              "      <td>-18.616150</td>\n",
              "      <td>-11.617200</td>\n",
              "      <td>-16.181297</td>\n",
              "      <td>...</td>\n",
              "      <td>-26.216665</td>\n",
              "      <td>-26.941952</td>\n",
              "      <td>-28.442326</td>\n",
              "      <td>-28.604067</td>\n",
              "      <td>-26.398375</td>\n",
              "      <td>-22.929688</td>\n",
              "      <td>-20.169323</td>\n",
              "      <td>-19.942419</td>\n",
              "      <td>-22.726204</td>\n",
              "      <td>-26.250223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-24.241844</td>\n",
              "      <td>-31.303322</td>\n",
              "      <td>-34.410553</td>\n",
              "      <td>-29.946802</td>\n",
              "      <td>-19.831509</td>\n",
              "      <td>-13.028464</td>\n",
              "      <td>-17.264351</td>\n",
              "      <td>-25.974705</td>\n",
              "      <td>-24.990423</td>\n",
              "      <td>-14.404683</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.381468</td>\n",
              "      <td>-22.464420</td>\n",
              "      <td>-24.520542</td>\n",
              "      <td>-25.629250</td>\n",
              "      <td>-22.467657</td>\n",
              "      <td>-18.252396</td>\n",
              "      <td>-18.697529</td>\n",
              "      <td>-23.529217</td>\n",
              "      <td>-27.686354</td>\n",
              "      <td>-28.613878</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 9060 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72dc2bd7-6f97-49a1-bdb5-e816991c092d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72dc2bd7-6f97-49a1-bdb5-e816991c092d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72dc2bd7-6f97-49a1-bdb5-e816991c092d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.00607193 -0.01121886 -0.00934151 ... -0.00844269 -0.00810874\n",
            "  -0.00596904]\n",
            " [ 0.0170495   0.00828338  0.00236239 ... -0.00221763 -0.00264256\n",
            "  -0.00534579]\n",
            " [ 0.01295445  0.00106594  0.00203606 ... -0.00396928 -0.00135212\n",
            "   0.00162666]\n",
            " ...\n",
            " [ 0.00867317  0.00394784  0.00386225 ... -0.00086329 -0.0009644\n",
            "   0.00138182]\n",
            " [ 0.00529683  0.00158674  0.00222707 ... -0.00057693 -0.00043765\n",
            "   0.00192815]\n",
            " [ 0.00240439 -0.00024603  0.00130026 ... -0.00131385 -0.00221978\n",
            "  -0.00069626]]\n",
            "[[-31.437506  -37.74198   -35.442417  ... -34.341454  -33.9324\n",
            "  -31.311481 ]\n",
            " [ -3.1160698 -13.853685  -21.106306  ... -26.716377  -27.236868\n",
            "  -30.548061 ]\n",
            " [ -8.132089  -22.694328  -21.506035  ... -28.861977  -25.656206\n",
            "  -22.007502 ]\n",
            " ...\n",
            " [-13.376234  -19.164291  -19.269133  ... -25.057444  -25.181297\n",
            "  -22.307407 ]\n",
            " [-17.51191   -22.056398  -21.272062  ... -24.706678  -24.536081\n",
            "  -21.63821  ]\n",
            " [-21.054865  -24.301363  -22.407312  ... -25.60933   -26.719006\n",
            "  -24.852844 ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    0     1     2     3     4     5     6     7     8     9     ...  9050  \\\n",
              "0    -29   -36   -27   -16   -21   -28   -19    -7   -15   -28  ...   -19   \n",
              "1    -35   -23   -17   -20   -26   -25   -19   -13   -11   -22  ...   -24   \n",
              "2    -19   -28   -29   -19   -18   -28   -32   -26   -25   -30  ...   -32   \n",
              "3    -31   -37   -35   -22   -10   -10   -16   -19   -19   -18  ...   -24   \n",
              "4    -31   -25   -30   -34   -23    -6    -3   -13   -22   -28  ...   -22   \n",
              "..   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
              "66   -16   -29   -25   -15   -24   -41   -34    -8    -1   -23  ...   -30   \n",
              "67   -18   -21   -30   -31   -27   -26   -30   -28   -15    -3  ...   -24   \n",
              "68   -33   -24   -16   -16   -22   -26   -29   -33   -31   -20  ...   -23   \n",
              "69   -37   -27   -19   -16   -19   -26   -27   -18   -11   -16  ...   -26   \n",
              "70   -24   -31   -34   -29   -19   -13   -17   -25   -24   -14  ...   -22   \n",
              "\n",
              "    9051  9052  9053  9054  9055  9056  9057  9058  9059  \n",
              "0    -16   -13   -11   -10   -10   -10   -10   -11   -15  \n",
              "1    -21   -20   -24   -28   -28   -27   -28   -28   -27  \n",
              "2    -28   -28   -28   -24   -22   -23   -25   -21   -17  \n",
              "3    -22   -21   -24   -25   -25   -24   -25   -26   -24  \n",
              "4    -20   -16   -16   -18   -17   -13   -13   -18   -23  \n",
              "..   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "66   -26   -26   -29   -30   -28   -26   -27   -28   -27  \n",
              "67   -25   -26   -27   -24   -20   -17   -17   -18   -20  \n",
              "68   -24   -24   -22   -19   -16   -18   -24   -29   -29  \n",
              "69   -26   -28   -28   -26   -22   -20   -19   -22   -26  \n",
              "70   -22   -24   -25   -22   -18   -18   -23   -27   -28  \n",
              "\n",
              "[71 rows x 9060 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b831e42-7e30-43a4-8ee9-a87073d91326\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9050</th>\n",
              "      <th>9051</th>\n",
              "      <th>9052</th>\n",
              "      <th>9053</th>\n",
              "      <th>9054</th>\n",
              "      <th>9055</th>\n",
              "      <th>9056</th>\n",
              "      <th>9057</th>\n",
              "      <th>9058</th>\n",
              "      <th>9059</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-27</td>\n",
              "      <td>-16</td>\n",
              "      <td>-21</td>\n",
              "      <td>-28</td>\n",
              "      <td>-19</td>\n",
              "      <td>-7</td>\n",
              "      <td>-15</td>\n",
              "      <td>-28</td>\n",
              "      <td>...</td>\n",
              "      <td>-19</td>\n",
              "      <td>-16</td>\n",
              "      <td>-13</td>\n",
              "      <td>-11</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-11</td>\n",
              "      <td>-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-35</td>\n",
              "      <td>-23</td>\n",
              "      <td>-17</td>\n",
              "      <td>-20</td>\n",
              "      <td>-26</td>\n",
              "      <td>-25</td>\n",
              "      <td>-19</td>\n",
              "      <td>-13</td>\n",
              "      <td>-11</td>\n",
              "      <td>-22</td>\n",
              "      <td>...</td>\n",
              "      <td>-24</td>\n",
              "      <td>-21</td>\n",
              "      <td>-20</td>\n",
              "      <td>-24</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-19</td>\n",
              "      <td>-28</td>\n",
              "      <td>-29</td>\n",
              "      <td>-19</td>\n",
              "      <td>-18</td>\n",
              "      <td>-28</td>\n",
              "      <td>-32</td>\n",
              "      <td>-26</td>\n",
              "      <td>-25</td>\n",
              "      <td>-30</td>\n",
              "      <td>...</td>\n",
              "      <td>-32</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-24</td>\n",
              "      <td>-22</td>\n",
              "      <td>-23</td>\n",
              "      <td>-25</td>\n",
              "      <td>-21</td>\n",
              "      <td>-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-31</td>\n",
              "      <td>-37</td>\n",
              "      <td>-35</td>\n",
              "      <td>-22</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-16</td>\n",
              "      <td>-19</td>\n",
              "      <td>-19</td>\n",
              "      <td>-18</td>\n",
              "      <td>...</td>\n",
              "      <td>-24</td>\n",
              "      <td>-22</td>\n",
              "      <td>-21</td>\n",
              "      <td>-24</td>\n",
              "      <td>-25</td>\n",
              "      <td>-25</td>\n",
              "      <td>-24</td>\n",
              "      <td>-25</td>\n",
              "      <td>-26</td>\n",
              "      <td>-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-31</td>\n",
              "      <td>-25</td>\n",
              "      <td>-30</td>\n",
              "      <td>-34</td>\n",
              "      <td>-23</td>\n",
              "      <td>-6</td>\n",
              "      <td>-3</td>\n",
              "      <td>-13</td>\n",
              "      <td>-22</td>\n",
              "      <td>-28</td>\n",
              "      <td>...</td>\n",
              "      <td>-22</td>\n",
              "      <td>-20</td>\n",
              "      <td>-16</td>\n",
              "      <td>-16</td>\n",
              "      <td>-18</td>\n",
              "      <td>-17</td>\n",
              "      <td>-13</td>\n",
              "      <td>-13</td>\n",
              "      <td>-18</td>\n",
              "      <td>-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-16</td>\n",
              "      <td>-29</td>\n",
              "      <td>-25</td>\n",
              "      <td>-15</td>\n",
              "      <td>-24</td>\n",
              "      <td>-41</td>\n",
              "      <td>-34</td>\n",
              "      <td>-8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-23</td>\n",
              "      <td>...</td>\n",
              "      <td>-30</td>\n",
              "      <td>-26</td>\n",
              "      <td>-26</td>\n",
              "      <td>-29</td>\n",
              "      <td>-30</td>\n",
              "      <td>-28</td>\n",
              "      <td>-26</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-18</td>\n",
              "      <td>-21</td>\n",
              "      <td>-30</td>\n",
              "      <td>-31</td>\n",
              "      <td>-27</td>\n",
              "      <td>-26</td>\n",
              "      <td>-30</td>\n",
              "      <td>-28</td>\n",
              "      <td>-15</td>\n",
              "      <td>-3</td>\n",
              "      <td>...</td>\n",
              "      <td>-24</td>\n",
              "      <td>-25</td>\n",
              "      <td>-26</td>\n",
              "      <td>-27</td>\n",
              "      <td>-24</td>\n",
              "      <td>-20</td>\n",
              "      <td>-17</td>\n",
              "      <td>-17</td>\n",
              "      <td>-18</td>\n",
              "      <td>-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-33</td>\n",
              "      <td>-24</td>\n",
              "      <td>-16</td>\n",
              "      <td>-16</td>\n",
              "      <td>-22</td>\n",
              "      <td>-26</td>\n",
              "      <td>-29</td>\n",
              "      <td>-33</td>\n",
              "      <td>-31</td>\n",
              "      <td>-20</td>\n",
              "      <td>...</td>\n",
              "      <td>-23</td>\n",
              "      <td>-24</td>\n",
              "      <td>-24</td>\n",
              "      <td>-22</td>\n",
              "      <td>-19</td>\n",
              "      <td>-16</td>\n",
              "      <td>-18</td>\n",
              "      <td>-24</td>\n",
              "      <td>-29</td>\n",
              "      <td>-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-37</td>\n",
              "      <td>-27</td>\n",
              "      <td>-19</td>\n",
              "      <td>-16</td>\n",
              "      <td>-19</td>\n",
              "      <td>-26</td>\n",
              "      <td>-27</td>\n",
              "      <td>-18</td>\n",
              "      <td>-11</td>\n",
              "      <td>-16</td>\n",
              "      <td>...</td>\n",
              "      <td>-26</td>\n",
              "      <td>-26</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-26</td>\n",
              "      <td>-22</td>\n",
              "      <td>-20</td>\n",
              "      <td>-19</td>\n",
              "      <td>-22</td>\n",
              "      <td>-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-24</td>\n",
              "      <td>-31</td>\n",
              "      <td>-34</td>\n",
              "      <td>-29</td>\n",
              "      <td>-19</td>\n",
              "      <td>-13</td>\n",
              "      <td>-17</td>\n",
              "      <td>-25</td>\n",
              "      <td>-24</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>-22</td>\n",
              "      <td>-22</td>\n",
              "      <td>-24</td>\n",
              "      <td>-25</td>\n",
              "      <td>-22</td>\n",
              "      <td>-18</td>\n",
              "      <td>-18</td>\n",
              "      <td>-23</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 9060 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b831e42-7e30-43a4-8ee9-a87073d91326')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b831e42-7e30-43a4-8ee9-a87073d91326 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b831e42-7e30-43a4-8ee9-a87073d91326');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Quantized output : \n",
            "\n",
            "[ -61   44 -122 -116]\n",
            "[ -61   44 -122 -116] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-123 -123  110 -120]\n",
            "[-123 -123  110 -120] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  32  -95  -87 -107]\n",
            "[  32  -95  -87 -107] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-122 -112 -125  102]\n",
            "[-122 -112 -125  102] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -92   34 -101  -98]\n",
            "[ -92   34 -101  -98] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-115 -122   94 -113]\n",
            "[-115 -122   94 -113] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  31  -64 -124  -98]\n",
            "[  31  -64 -124  -98] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-124  111 -121 -123]\n",
            "[-124  111 -121 -123] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-122 -123  109 -120]\n",
            "[-122 -123  109 -120] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  87 -110 -112 -122]\n",
            "[  87 -110 -112 -122] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[ -99 -114 -105   62]\n",
            "[ -99 -114 -105   62] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -45  -22 -120  -69]\n",
            "[ -45  -22 -120  -69] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-117 -126  112 -125]\n",
            "[-117 -126  112 -125] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  97 -112 -117 -124]\n",
            "[  97 -112 -117 -124] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-123 -124 -114  105]\n",
            "[-123 -124 -114  105] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-106   75 -111 -114]\n",
            "[-106   75 -111 -114] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-118 -121   92 -109]\n",
            "[-118 -121   92 -109] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[-40 -95 -54 -66]\n",
            "[-40 -95 -54 -66] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-127 -127 -126  124]\n",
            "[-127 -127 -126  124] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -87  -74 -125   30]\n",
            "[ -87  -74 -125   30] [3]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-122 -123   92 -103]\n",
            "[-122 -123   92 -103] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  69 -108 -111 -105]\n",
            "[  69 -108 -111 -105] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-124 -123 -121  112]\n",
            "[-124 -123 -121  112] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-121   74 -109 -100]\n",
            "[-121   74 -109 -100] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-100 -101   40  -96]\n",
            "[-100 -101   40  -96] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[-127 -127 -127  125]\n",
            "[-127 -127 -127  125] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-124  109 -118 -123]\n",
            "[-124  109 -118 -123] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-128 -126  124 -126]\n",
            "[-128 -126  124 -126] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  74  -86 -124 -120]\n",
            "[  74  -86 -124 -120] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-116 -114 -116   90]\n",
            "[-116 -114 -116   90] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[  14  -55 -110 -105]\n",
            "[  14  -55 -110 -105] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-125 -121  110 -120]\n",
            "[-125 -121  110 -120] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  57  -96 -114 -104]\n",
            "[  57  -96 -114 -104] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-99 -72 -90   5]\n",
            "[-99 -72 -90   5] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-126  117 -123 -124]\n",
            "[-126  117 -123 -124] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-110 -119   89 -117]\n",
            "[-110 -119   89 -117] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[ 119 -123 -126 -127]\n",
            "[ 119 -123 -126 -127] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-120 -100 -121   85]\n",
            "[-120 -100 -121   85] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -88   41 -109 -100]\n",
            "[ -88   41 -109 -100] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-110 -124   89 -110]\n",
            "[-110 -124   89 -110] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[ -37  -24 -114  -81]\n",
            "[ -37  -24 -114  -81] [1]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[ -81 -101  -87   12]\n",
            "[ -81 -101  -87   12] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -41   -9 -117  -89]\n",
            "[ -41   -9 -117  -89] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-110 -108  -52   14]\n",
            "[-110 -108  -52   14] [3]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[ 117 -123 -126 -124]\n",
            "[ 117 -123 -126 -124] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[ -95 -109 -113   61]\n",
            "[ -95 -109 -113   61] [3]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[ -40  -68 -108  -40]\n",
            "[ -40  -68 -108  -40] [0 3]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-127 -112 -120  104]\n",
            "[-127 -112 -120  104] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-123  113 -123 -123]\n",
            "[-123  113 -123 -123] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-124 -127  121 -126]\n",
            "[-124 -127  121 -126] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  85 -107 -120 -113]\n",
            "[  85 -107 -120 -113] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-127 -125 -124  120]\n",
            "[-127 -125 -124  120] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -42  -18  -96 -101]\n",
            "[ -42  -18  -96 -101] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[ -91 -119   62 -108]\n",
            "[ -91 -119   62 -108] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[ 104 -124 -124 -112]\n",
            "[ 104 -124 -124 -112] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-128 -127 -128  127]\n",
            "[-128 -127 -128  127] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -40   -3 -105 -108]\n",
            "[ -40   -3 -105 -108] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[ -89 -109   45 -103]\n",
            "[ -89 -109   45 -103] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  54  -95 -120  -95]\n",
            "[  54  -95 -120  -95] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-107 -106 -113   71]\n",
            "[-107 -106 -113   71] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[ -77   43 -115 -107]\n",
            "[ -77   43 -115 -107] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-127 -128  125 -127]\n",
            "[-127 -128  125 -127] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  20  -44 -123 -109]\n",
            "[  20  -44 -123 -109] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-125 -113 -121  103]\n",
            "[-125 -113 -121  103] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[  68  -87 -119 -118]\n",
            "[  68  -87 -119 -118] [0]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[  70  -93 -115 -117]\n",
            "[  70  -93 -115 -117] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-127 -127 -127  125]\n",
            "[-127 -127 -127  125] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "[-68 -27 -98 -64]\n",
            "[-68 -27 -98 -64] [1]\n",
            "[0. 1. 0. 0.] [1]\n",
            "[-122 -116  104 -122]\n",
            "[-122 -116  104 -122] [2]\n",
            "[0. 0. 1. 0.] [2]\n",
            "[  92 -113 -115 -120]\n",
            "[  92 -113 -115 -120] [0]\n",
            "[1. 0. 0. 0.] [0]\n",
            "[-126 -123 -125  118]\n",
            "[-126 -123 -125  118] [3]\n",
            "[0. 0. 0. 1.] [3]\n",
            "Classification Accuracy = %f\n",
            " 0.9014084507042254\n",
            "[0. 1. 0. 0.] [0.26171875 0.671875   0.0234375  0.046875  ]\n",
            "[0. 0. 1. 0.] [0.01953125 0.01953125 0.9296875  0.03125   ]\n",
            "[1. 0. 0. 0.] [0.625      0.12890625 0.16015625 0.08203125]\n",
            "[0. 0. 0. 1.] [0.0234375  0.0625     0.01171875 0.8984375 ]\n",
            "[0. 1. 0. 0.] [0.140625   0.6328125  0.10546875 0.1171875 ]\n",
            "[0. 0. 1. 0.] [0.05078125 0.0234375  0.8671875  0.05859375]\n",
            "[1. 0. 0. 0.] [0.62109375 0.25       0.015625   0.1171875 ]\n",
            "[0. 1. 0. 0.] [0.015625   0.93359375 0.02734375 0.01953125]\n",
            "[0. 0. 1. 0.] [0.0234375  0.01953125 0.92578125 0.03125   ]\n",
            "[1. 0. 0. 0.] [0.83984375 0.0703125  0.0625     0.0234375 ]\n",
            "[0. 0. 0. 1.] [0.11328125 0.0546875  0.08984375 0.7421875 ]\n",
            "[0. 1. 0. 0.] [0.32421875 0.4140625  0.03125    0.23046875]\n",
            "[0. 0. 1. 0.] [0.04296875 0.0078125  0.9375     0.01171875]\n",
            "[1. 0. 0. 0.] [0.87890625 0.0625     0.04296875 0.015625  ]\n",
            "[0. 0. 0. 1.] [0.01953125 0.015625   0.0546875  0.91015625]\n",
            "[0. 1. 0. 0.] [0.0859375  0.79296875 0.06640625 0.0546875 ]\n",
            "[0. 0. 1. 0.] [0.0390625  0.02734375 0.859375   0.07421875]\n",
            "[1. 0. 0. 0.] [0.34375    0.12890625 0.2890625  0.2421875 ]\n",
            "[0. 0. 0. 1.] [0.00390625 0.00390625 0.0078125  0.984375  ]\n",
            "[0. 1. 0. 0.] [0.16015625 0.2109375  0.01171875 0.6171875 ]\n",
            "[0. 0. 1. 0.] [0.0234375  0.01953125 0.859375   0.09765625]\n",
            "[1. 0. 0. 0.] [0.76953125 0.078125   0.06640625 0.08984375]\n",
            "[0. 0. 0. 1.] [0.015625   0.01953125 0.02734375 0.9375    ]\n",
            "[0. 1. 0. 0.] [0.02734375 0.7890625  0.07421875 0.109375  ]\n",
            "[0. 0. 1. 0.] [0.109375   0.10546875 0.65625    0.125     ]\n",
            "[0. 0. 0. 1.] [0.00390625 0.00390625 0.00390625 0.98828125]\n",
            "[0. 1. 0. 0.] [0.015625   0.92578125 0.0390625  0.01953125]\n",
            "[0. 0. 1. 0.] [0.        0.0078125 0.984375  0.0078125]\n",
            "[1. 0. 0. 0.] [0.7890625 0.1640625 0.015625  0.03125  ]\n",
            "[0. 0. 0. 1.] [0.046875  0.0546875 0.046875  0.8515625]\n",
            "[0. 1. 0. 0.] [0.5546875  0.28515625 0.0703125  0.08984375]\n",
            "[0. 0. 1. 0.] [0.01171875 0.02734375 0.9296875  0.03125   ]\n",
            "[1. 0. 0. 0.] [0.72265625 0.125      0.0546875  0.09375   ]\n",
            "[0. 0. 0. 1.] [0.11328125 0.21875    0.1484375  0.51953125]\n",
            "[0. 1. 0. 0.] [0.0078125  0.95703125 0.01953125 0.015625  ]\n",
            "[0. 0. 1. 0.] [0.0703125  0.03515625 0.84765625 0.04296875]\n",
            "[1. 0. 0. 0.] [0.96484375 0.01953125 0.0078125  0.00390625]\n",
            "[0. 0. 0. 1.] [0.03125    0.109375   0.02734375 0.83203125]\n",
            "[0. 1. 0. 0.] [0.15625    0.66015625 0.07421875 0.109375  ]\n",
            "[0. 0. 1. 0.] [0.0703125  0.015625   0.84765625 0.0703125 ]\n",
            "[1. 0. 0. 0.] [0.35546875 0.40625    0.0546875  0.18359375]\n",
            "[0. 0. 0. 1.] [0.18359375 0.10546875 0.16015625 0.546875  ]\n",
            "[0. 1. 0. 0.] [0.33984375 0.46484375 0.04296875 0.15234375]\n",
            "[0. 0. 1. 0.] [0.0703125 0.078125  0.296875  0.5546875]\n",
            "[0. 1. 0. 0.] [0.95703125 0.01953125 0.0078125  0.015625  ]\n",
            "[0. 0. 1. 0.] [0.12890625 0.07421875 0.05859375 0.73828125]\n",
            "[1. 0. 0. 0.] [0.34375  0.234375 0.078125 0.34375 ]\n",
            "[0. 0. 0. 1.] [0.00390625 0.0625     0.03125    0.90625   ]\n",
            "[0. 1. 0. 0.] [0.01953125 0.94140625 0.01953125 0.01953125]\n",
            "[0. 0. 1. 0.] [0.015625   0.00390625 0.97265625 0.0078125 ]\n",
            "[1. 0. 0. 0.] [0.83203125 0.08203125 0.03125    0.05859375]\n",
            "[0. 0. 0. 1.] [0.00390625 0.01171875 0.015625   0.96875   ]\n",
            "[0. 1. 0. 0.] [0.3359375  0.4296875  0.125      0.10546875]\n",
            "[0. 0. 1. 0.] [0.14453125 0.03515625 0.7421875  0.078125  ]\n",
            "[1. 0. 0. 0.] [0.90625  0.015625 0.015625 0.0625  ]\n",
            "[0. 0. 0. 1.] [0.         0.00390625 0.         0.99609375]\n",
            "[0. 1. 0. 0.] [0.34375    0.48828125 0.08984375 0.078125  ]\n",
            "[0. 0. 1. 0.] [0.15234375 0.07421875 0.67578125 0.09765625]\n",
            "[1. 0. 0. 0.] [0.7109375  0.12890625 0.03125    0.12890625]\n",
            "[0. 0. 0. 1.] [0.08203125 0.0859375  0.05859375 0.77734375]\n",
            "[0. 1. 0. 0.] [0.19921875 0.66796875 0.05078125 0.08203125]\n",
            "[0. 0. 1. 0.] [0.00390625 0.         0.98828125 0.00390625]\n",
            "[1. 0. 0. 0.] [0.578125   0.328125   0.01953125 0.07421875]\n",
            "[0. 0. 0. 1.] [0.01171875 0.05859375 0.02734375 0.90234375]\n",
            "[0. 1. 0. 0.] [0.765625   0.16015625 0.03515625 0.0390625 ]\n",
            "[1. 0. 0. 0.] [0.7734375  0.13671875 0.05078125 0.04296875]\n",
            "[0. 0. 0. 1.] [0.00390625 0.00390625 0.00390625 0.98828125]\n",
            "[0. 1. 0. 0.] [0.234375   0.39453125 0.1171875  0.25      ]\n",
            "[0. 0. 1. 0.] [0.0234375 0.046875  0.90625   0.0234375]\n",
            "[1. 0. 0. 0.] [0.859375   0.05859375 0.05078125 0.03125   ]\n",
            "[0. 0. 0. 1.] [0.0078125  0.01953125 0.01171875 0.9609375 ]\n",
            "\n",
            "\n",
            "Classification Accuracy = %f\n",
            " 0.9014084507042254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test_.shape)\n",
        "x_test2 = np.reshape(x_test_,(71,60*151));\n",
        "print(x_test2.shape)\n",
        "\n",
        "dt = pd.DataFrame(x_test2);\n",
        "dt[['y1','y2','y3','y4']] = Y_test;\n",
        "display(dt)\n",
        "dt.to_csv('quantized_int8_Xtest.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "MNL5vFD05tOk",
        "outputId": "29a19793-c60a-4e12-f763-7decd1fa163e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(71, 60, 151)\n",
            "(71, 9060)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     0   1   2   3   4   5   6   7   8   9  ...  9054  9055  9056  9057  9058  \\\n",
              "0  -29 -36 -27 -16 -21 -28 -19  -7 -15 -28  ...   -10   -10   -10   -10   -11   \n",
              "1  -35 -23 -17 -20 -26 -25 -19 -13 -11 -22  ...   -28   -28   -27   -28   -28   \n",
              "2  -19 -28 -29 -19 -18 -28 -32 -26 -25 -30  ...   -24   -22   -23   -25   -21   \n",
              "3  -31 -37 -35 -22 -10 -10 -16 -19 -19 -18  ...   -25   -25   -24   -25   -26   \n",
              "4  -31 -25 -30 -34 -23  -6  -3 -13 -22 -28  ...   -18   -17   -13   -13   -18   \n",
              "..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...   ...   ...   ...   ...   ...   \n",
              "66 -16 -29 -25 -15 -24 -41 -34  -8  -1 -23  ...   -30   -28   -26   -27   -28   \n",
              "67 -18 -21 -30 -31 -27 -26 -30 -28 -15  -3  ...   -24   -20   -17   -17   -18   \n",
              "68 -33 -24 -16 -16 -22 -26 -29 -33 -31 -20  ...   -19   -16   -18   -24   -29   \n",
              "69 -37 -27 -19 -16 -19 -26 -27 -18 -11 -16  ...   -26   -22   -20   -19   -22   \n",
              "70 -24 -31 -34 -29 -19 -13 -17 -25 -24 -14  ...   -22   -18   -18   -23   -27   \n",
              "\n",
              "    9059   y1   y2   y3   y4  \n",
              "0    -15  0.0  1.0  0.0  0.0  \n",
              "1    -27  0.0  0.0  1.0  0.0  \n",
              "2    -17  1.0  0.0  0.0  0.0  \n",
              "3    -24  0.0  0.0  0.0  1.0  \n",
              "4    -23  0.0  1.0  0.0  0.0  \n",
              "..   ...  ...  ...  ...  ...  \n",
              "66   -27  0.0  0.0  0.0  1.0  \n",
              "67   -20  0.0  1.0  0.0  0.0  \n",
              "68   -29  0.0  0.0  1.0  0.0  \n",
              "69   -26  1.0  0.0  0.0  0.0  \n",
              "70   -28  0.0  0.0  0.0  1.0  \n",
              "\n",
              "[71 rows x 9064 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35e6c909-9bea-457f-99bd-2663fd3bcf01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9054</th>\n",
              "      <th>9055</th>\n",
              "      <th>9056</th>\n",
              "      <th>9057</th>\n",
              "      <th>9058</th>\n",
              "      <th>9059</th>\n",
              "      <th>y1</th>\n",
              "      <th>y2</th>\n",
              "      <th>y3</th>\n",
              "      <th>y4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-27</td>\n",
              "      <td>-16</td>\n",
              "      <td>-21</td>\n",
              "      <td>-28</td>\n",
              "      <td>-19</td>\n",
              "      <td>-7</td>\n",
              "      <td>-15</td>\n",
              "      <td>-28</td>\n",
              "      <td>...</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-11</td>\n",
              "      <td>-15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-35</td>\n",
              "      <td>-23</td>\n",
              "      <td>-17</td>\n",
              "      <td>-20</td>\n",
              "      <td>-26</td>\n",
              "      <td>-25</td>\n",
              "      <td>-19</td>\n",
              "      <td>-13</td>\n",
              "      <td>-11</td>\n",
              "      <td>-22</td>\n",
              "      <td>...</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-19</td>\n",
              "      <td>-28</td>\n",
              "      <td>-29</td>\n",
              "      <td>-19</td>\n",
              "      <td>-18</td>\n",
              "      <td>-28</td>\n",
              "      <td>-32</td>\n",
              "      <td>-26</td>\n",
              "      <td>-25</td>\n",
              "      <td>-30</td>\n",
              "      <td>...</td>\n",
              "      <td>-24</td>\n",
              "      <td>-22</td>\n",
              "      <td>-23</td>\n",
              "      <td>-25</td>\n",
              "      <td>-21</td>\n",
              "      <td>-17</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-31</td>\n",
              "      <td>-37</td>\n",
              "      <td>-35</td>\n",
              "      <td>-22</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-16</td>\n",
              "      <td>-19</td>\n",
              "      <td>-19</td>\n",
              "      <td>-18</td>\n",
              "      <td>...</td>\n",
              "      <td>-25</td>\n",
              "      <td>-25</td>\n",
              "      <td>-24</td>\n",
              "      <td>-25</td>\n",
              "      <td>-26</td>\n",
              "      <td>-24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-31</td>\n",
              "      <td>-25</td>\n",
              "      <td>-30</td>\n",
              "      <td>-34</td>\n",
              "      <td>-23</td>\n",
              "      <td>-6</td>\n",
              "      <td>-3</td>\n",
              "      <td>-13</td>\n",
              "      <td>-22</td>\n",
              "      <td>-28</td>\n",
              "      <td>...</td>\n",
              "      <td>-18</td>\n",
              "      <td>-17</td>\n",
              "      <td>-13</td>\n",
              "      <td>-13</td>\n",
              "      <td>-18</td>\n",
              "      <td>-23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-16</td>\n",
              "      <td>-29</td>\n",
              "      <td>-25</td>\n",
              "      <td>-15</td>\n",
              "      <td>-24</td>\n",
              "      <td>-41</td>\n",
              "      <td>-34</td>\n",
              "      <td>-8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-23</td>\n",
              "      <td>...</td>\n",
              "      <td>-30</td>\n",
              "      <td>-28</td>\n",
              "      <td>-26</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "      <td>-27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-18</td>\n",
              "      <td>-21</td>\n",
              "      <td>-30</td>\n",
              "      <td>-31</td>\n",
              "      <td>-27</td>\n",
              "      <td>-26</td>\n",
              "      <td>-30</td>\n",
              "      <td>-28</td>\n",
              "      <td>-15</td>\n",
              "      <td>-3</td>\n",
              "      <td>...</td>\n",
              "      <td>-24</td>\n",
              "      <td>-20</td>\n",
              "      <td>-17</td>\n",
              "      <td>-17</td>\n",
              "      <td>-18</td>\n",
              "      <td>-20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>-33</td>\n",
              "      <td>-24</td>\n",
              "      <td>-16</td>\n",
              "      <td>-16</td>\n",
              "      <td>-22</td>\n",
              "      <td>-26</td>\n",
              "      <td>-29</td>\n",
              "      <td>-33</td>\n",
              "      <td>-31</td>\n",
              "      <td>-20</td>\n",
              "      <td>...</td>\n",
              "      <td>-19</td>\n",
              "      <td>-16</td>\n",
              "      <td>-18</td>\n",
              "      <td>-24</td>\n",
              "      <td>-29</td>\n",
              "      <td>-29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-37</td>\n",
              "      <td>-27</td>\n",
              "      <td>-19</td>\n",
              "      <td>-16</td>\n",
              "      <td>-19</td>\n",
              "      <td>-26</td>\n",
              "      <td>-27</td>\n",
              "      <td>-18</td>\n",
              "      <td>-11</td>\n",
              "      <td>-16</td>\n",
              "      <td>...</td>\n",
              "      <td>-26</td>\n",
              "      <td>-22</td>\n",
              "      <td>-20</td>\n",
              "      <td>-19</td>\n",
              "      <td>-22</td>\n",
              "      <td>-26</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-24</td>\n",
              "      <td>-31</td>\n",
              "      <td>-34</td>\n",
              "      <td>-29</td>\n",
              "      <td>-19</td>\n",
              "      <td>-13</td>\n",
              "      <td>-17</td>\n",
              "      <td>-25</td>\n",
              "      <td>-24</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>-22</td>\n",
              "      <td>-18</td>\n",
              "      <td>-18</td>\n",
              "      <td>-23</td>\n",
              "      <td>-27</td>\n",
              "      <td>-28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71 rows × 9064 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35e6c909-9bea-457f-99bd-2663fd3bcf01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35e6c909-9bea-457f-99bd-2663fd3bcf01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35e6c909-9bea-457f-99bd-2663fd3bcf01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hex file generation of model files**"
      ],
      "metadata": {
        "id": "x8XNtt3Nf3g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Another Way of getting the array in .h file\n",
        "def hex_to_c_array(hex_data, var_name):\n",
        "\n",
        "  c_str = ''\n",
        "\n",
        "  # Create header guard\n",
        "  c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
        "  c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
        "\n",
        "  # Add array length at top of file\n",
        "  c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
        "\n",
        "  # Declare C variable\n",
        "  c_str += 'unsigned char ' + var_name + '[] = {'\n",
        "  hex_array = []\n",
        "  for i, val in enumerate(hex_data) :\n",
        "\n",
        "    # Construct string from hex\n",
        "    hex_str = format(val, '#04x')\n",
        "\n",
        "    # Add formatting so each line stays within 80 characters\n",
        "    if (i + 1) < len(hex_data):\n",
        "      hex_str += ','\n",
        "    if (i + 1) % 12 == 0:\n",
        "      hex_str += '\\n '\n",
        "    hex_array.append(hex_str)\n",
        "\n",
        "  # Add closing brace\n",
        "  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
        "\n",
        "  # Close out header guard\n",
        "  c_str += '#endif //' + var_name.upper() + '_H'\n",
        "\n",
        "  return c_str"
      ],
      "metadata": {
        "id": "zha6YobtW64u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting non-quantized model hex file**"
      ],
      "metadata": {
        "id": "uUqpywC_gBTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_model_name = \"model_tflite\"\n",
        "with open(c_model_name + '.h', 'w') as file:\n",
        "  file.write(hex_to_c_array(model_tflite , c_model_name)) #model_tflite"
      ],
      "metadata": {
        "id": "-XY1Rgq0W7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting quantized model hex file**"
      ],
      "metadata": {
        "id": "dVu_LLakgNyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_model_name = \"model_tflite_float\"\n",
        "with open(c_model_name + '.h', 'w') as file:\n",
        "  file.write(hex_to_c_array(model_no_quant_tflite, c_model_name))"
      ],
      "metadata": {
        "id": "-vntObuPW86e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}